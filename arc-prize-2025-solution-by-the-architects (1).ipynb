{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13189cc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T02:27:57.469632Z",
     "iopub.status.busy": "2025-07-07T02:27:57.468967Z",
     "iopub.status.idle": "2025-07-07T02:27:57.472946Z",
     "shell.execute_reply": "2025-07-07T02:27:57.472356Z"
    },
    "papermill": {
     "duration": 0.014216,
     "end_time": "2025-07-07T02:27:57.474422",
     "exception": false,
     "start_time": "2025-07-07T02:27:57.460206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Daniel Franzen and Jan Disselhoff\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99d69710",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T02:27:57.488706Z",
     "iopub.status.busy": "2025-07-07T02:27:57.488228Z",
     "iopub.status.idle": "2025-07-07T02:27:57.490989Z",
     "shell.execute_reply": "2025-07-07T02:27:57.490445Z"
    },
    "papermill": {
     "duration": 0.011191,
     "end_time": "2025-07-07T02:27:57.492368",
     "exception": false,
     "start_time": "2025-07-07T02:27:57.481177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This notebook contains our winning submission to the ARC Prize 2025 Kaggle competition,\n",
    "# scoring 53.5 points on the private evaluation set.\n",
    "# the ARChitects (Daniel Franzen and Jan Disselhoff)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b71fb1",
   "metadata": {
    "papermill": {
     "duration": 0.006226,
     "end_time": "2025-07-07T02:27:57.505069",
     "exception": false,
     "start_time": "2025-07-07T02:27:57.498843",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "ARC2024 1등 code를 수정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505a22d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T02:27:57.519989Z",
     "iopub.status.busy": "2025-07-07T02:27:57.519749Z",
     "iopub.status.idle": "2025-07-07T02:27:57.555083Z",
     "shell.execute_reply": "2025-07-07T02:27:57.554508Z"
    },
    "papermill": {
     "duration": 0.045064,
     "end_time": "2025-07-07T02:27:57.556570",
     "exception": false,
     "start_time": "2025-07-07T02:27:57.511506",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile model_runner.py\n",
    "import json\n",
    "import os, sys\n",
    "import bz2\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def indices_required_for_merges(keep_indices, vocab, merges):\n",
    "    \"\"\"\n",
    "    BPE 병합에 필요한 모든 토큰 인덱스를 찾는 함수\n",
    "    \n",
    "    Args:\n",
    "        keep_indices: 유지할 토큰 인덱스들의 딕셔너리\n",
    "        vocab: 어휘 사전\n",
    "        merges: BPE 병합 규칙들\n",
    "    \n",
    "    Returns:\n",
    "        병합에 필요한 모든 인덱스가 포함된 딕셔너리\n",
    "    \"\"\"\n",
    "    merges_lookup = {}\n",
    "    # 각 병합 규칙에서 필요한 하위 토큰들을 매핑\n",
    "    for m in merges:\n",
    "        a, b = m.split(' ') if isinstance(m, str) else m\n",
    "        key = vocab[f'{a}{b}']\n",
    "        if key not in merges_lookup: merges_lookup[key] = set()\n",
    "        merges_lookup[key].add(vocab[a])\n",
    "        merges_lookup[key].add(vocab[b])\n",
    "    \n",
    "    # 재귀적으로 필요한 모든 토큰 인덱스 수집\n",
    "    to_process = list(keep_indices)\n",
    "    while len(to_process):\n",
    "        for w in merges_lookup.get(to_process.pop(), []):\n",
    "            if w not in keep_indices:\n",
    "                keep_indices[w] = None\n",
    "                to_process.append(w)\n",
    "    return keep_indices\n",
    "\n",
    "def remove_unused_merges(merges, vocab):\n",
    "    \"\"\"\n",
    "    사용되지 않는 BPE 병합 규칙들을 제거하는 함수\n",
    "    \n",
    "    Args:\n",
    "        merges: 병합 규칙 리스트\n",
    "        vocab: 어휘 사전\n",
    "    \n",
    "    Returns:\n",
    "        유효한 병합 규칙들만 포함된 리스트\n",
    "    \"\"\"\n",
    "    return [f'{a} {b}' for a, b in [m.split(' ') if isinstance(m, str) else m for m in merges] \n",
    "            if all(w in vocab for w in [a, b, a + b])]\n",
    "\n",
    "def map_special_tokens(data, mapping=None):\n",
    "    \"\"\"\n",
    "    특별 토큰들을 매핑하거나 수집하는 함수\n",
    "    \n",
    "    Args:\n",
    "        data: 토큰 데이터 (딕셔너리 또는 리스트)\n",
    "        mapping: 토큰 인덱스 매핑 (선택사항)\n",
    "    \n",
    "    Returns:\n",
    "        특별 토큰 인덱스들의 집합\n",
    "    \"\"\"\n",
    "    tokens = set()\n",
    "    if isinstance(data, dict):\n",
    "        special = data.get('special_tokens')\n",
    "        if special is not None:\n",
    "            for v in special.values():\n",
    "                tokens.update(v['ids'])\n",
    "                # 매핑이 제공된 경우 토큰 ID들을 새로운 인덱스로 변환\n",
    "                if mapping is not None:\n",
    "                    v['ids'] = [mapping.get(i) for i in v['ids'] if i in mapping]\n",
    "    \n",
    "    # 재귀적으로 중첩된 데이터 구조 처리\n",
    "    for v in (data.values() if isinstance(data, dict) else data if isinstance(data, list) else []):\n",
    "        tokens.update(map_special_tokens(v, mapping))\n",
    "    return tokens\n",
    "\n",
    "def remove_tokenizer_normalizer(tokenizer):\n",
    "    \"\"\"\n",
    "    토크나이저의 정규화 기능을 제거하는 함수\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: HuggingFace 토크나이저\n",
    "    \"\"\"\n",
    "    from tokenizers import Tokenizer\n",
    "    assert tokenizer.is_fast\n",
    "    tokenizer_json = json.loads(tokenizer._tokenizer.to_str())\n",
    "    if tokenizer_json.get('normalizer') is not None:\n",
    "        tokenizer_json['normalizer'] = None\n",
    "        tokenizer._tokenizer = Tokenizer.from_str(json.dumps(tokenizer_json))\n",
    "\n",
    "def shrink_tokenizer_vocab(tokenizer, keep_indices, keep_special_tokens, keep_token_order):\n",
    "    \"\"\"\n",
    "    토크나이저의 어휘를 축소하는 함수\n",
    "    \n",
    "    Args:\n",
    "        tokenizer: 토크나이저 객체\n",
    "        keep_indices: 유지할 토큰 인덱스들\n",
    "        keep_special_tokens: 특별 토큰 유지 여부\n",
    "        keep_token_order: 토큰 순서 유지 여부\n",
    "    \n",
    "    Returns:\n",
    "        (매핑 딕셔너리, 유지된 인덱스들)\n",
    "    \"\"\"\n",
    "    from tokenizers import Tokenizer\n",
    "    assert tokenizer.is_fast\n",
    "    tokenizer_json = json.loads(tokenizer._tokenizer.to_str())\n",
    "    assert tokenizer_json['model']['type'] == \"BPE\"\n",
    "    \n",
    "    # 특별 토큰들 추가\n",
    "    if keep_special_tokens:\n",
    "        keep_indices.update({k: None for k in tokenizer.all_special_ids})\n",
    "        keep_indices.update({k: None for k in map_special_tokens(tokenizer_json.get('post_processor'))})\n",
    "    \n",
    "    # BPE 병합에 필요한 모든 인덱스 포함\n",
    "    keep_indices = indices_required_for_merges(keep_indices, tokenizer_json['model']['vocab'], tokenizer_json['model']['merges'])\n",
    "    \n",
    "    # 토큰 순서 정렬\n",
    "    if keep_token_order: keep_indices = sorted(keep_indices)\n",
    "    \n",
    "    # 새로운 인덱스 매핑 생성\n",
    "    mapping = {old: new for new, old in enumerate(keep_indices)}\n",
    "    \n",
    "    # 어휘 사전 업데이트\n",
    "    tokenizer_json['model']['vocab'] = {k: mapping[v] for k, v in tokenizer_json['model']['vocab'].items() if v in mapping}\n",
    "    tokenizer_json['model']['merges'] = remove_unused_merges(tokenizer_json['model']['merges'], tokenizer_json['model']['vocab'])\n",
    "    \n",
    "    # 추가된 토큰들 업데이트\n",
    "    special_tokens_order = [t['id'] for t in tokenizer_json['added_tokens']]\n",
    "    assert special_tokens_order==sorted(special_tokens_order)\n",
    "    tokenizer_json['added_tokens'] = sorted([{**t, 'id': mapping[t['id']]} for t in tokenizer_json['added_tokens'] if t['id'] in mapping], key=lambda t: t['id'])\n",
    "    \n",
    "    # 후처리기 업데이트\n",
    "    map_special_tokens(tokenizer_json.get('post_processor'), mapping)\n",
    "    tokenizer._tokenizer = Tokenizer.from_str(json.dumps(tokenizer_json))\n",
    "    return mapping, keep_indices\n",
    "\n",
    "def shrink_model_embeddings(model, keep_indices, mapping):\n",
    "    \"\"\"\n",
    "    모델의 임베딩 레이어를 축소하는 함수\n",
    "    \n",
    "    Args:\n",
    "        model: 언어 모델\n",
    "        keep_indices: 유지할 인덱스들\n",
    "        mapping: 토큰 인덱스 매핑\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    with torch.no_grad():\n",
    "        # 유지할 토큰들만 선택\n",
    "        row_select = torch.tensor(list(keep_indices))\n",
    "        \n",
    "        # 입력 임베딩 축소\n",
    "        new_embed_t = torch.index_select(model.get_input_embeddings().weight.data, 0, row_select.to(model.get_input_embeddings().weight.data.device))\n",
    "        # 출력 임베딩 축소\n",
    "        new_lm_head = torch.index_select(model.get_output_embeddings().weight.data, 0, row_select.to(model.get_output_embeddings().weight.data.device))\n",
    "        \n",
    "        # 모델 크기 조정\n",
    "        model.resize_token_embeddings(len(keep_indices))\n",
    "        model.get_input_embeddings().weight.data[:] = new_embed_t\n",
    "        model.get_output_embeddings().weight.data[:] = new_lm_head\n",
    "        \n",
    "        # 모델 설정의 특별 토큰 ID들 업데이트\n",
    "        for config in [model.config, model.generation_config]:\n",
    "            for k, v in list(config.to_dict().items()):\n",
    "                if k.endswith('token_id'):\n",
    "                    setattr(config, k, [mapping.get(t) for t in v] if isinstance(v, list) else mapping.get(v))\n",
    "\n",
    "def shrink_embeddings(model, tokenizer, corpus=None, keep_token_ids=[], keep_tokens=[], remove_token_ids=[], keep_model_tokens=True, keep_special_tokens=True, keep_normalizer=False, keep_token_order=True):\n",
    "    \"\"\"\n",
    "    모델과 토크나이저의 임베딩을 축소하는 메인 함수\n",
    "    \n",
    "    Args:\n",
    "        model: 언어 모델\n",
    "        tokenizer: 토크나이저\n",
    "        corpus: 분석할 코퍼스 (선택사항)\n",
    "        keep_token_ids: 유지할 토큰 ID 리스트\n",
    "        keep_tokens: 유지할 토큰 문자열 리스트\n",
    "        remove_token_ids: 제거할 토큰 ID 리스트\n",
    "        keep_model_tokens: 모델 토큰 유지 여부\n",
    "        keep_special_tokens: 특별 토큰 유지 여부\n",
    "        keep_normalizer: 정규화기 유지 여부\n",
    "        keep_token_order: 토큰 순서 유지 여부\n",
    "    \n",
    "    Returns:\n",
    "        토큰 인덱스 매핑 딕셔너리\n",
    "    \"\"\"\n",
    "    if not keep_normalizer: remove_tokenizer_normalizer(tokenizer)\n",
    "    from collections import OrderedDict  # 순서가 있는 집합으로 사용\n",
    "    keep_indices = OrderedDict()\n",
    "    \n",
    "    # 유지할 토큰들 수집\n",
    "    keep_indices.update({k: None for k in keep_token_ids})\n",
    "    keep_indices.update({tokenizer.vocab[t]: None for t in keep_tokens})\n",
    "    if corpus is not None: keep_indices.update({k: None for k in tokenizer(corpus)['input_ids']})\n",
    "    \n",
    "    # 모델에서 사용되는 토큰들 유지\n",
    "    if keep_model_tokens:\n",
    "        for config in [model.config, model.generation_config]:\n",
    "            for k, v in config.to_dict().items():\n",
    "                if k.endswith('token_id'):\n",
    "                    keep_indices.update({k: None for k in (v if isinstance(v, list) else [v])})\n",
    "    \n",
    "    # None 값과 제거할 토큰들 정리\n",
    "    keep_indices.pop(None, None)\n",
    "    for idx in remove_token_ids: keep_indices.pop(idx, None)\n",
    "    \n",
    "    # 토크나이저와 모델 축소 실행\n",
    "    mapping, keep_indices = shrink_tokenizer_vocab(tokenizer, keep_indices, keep_special_tokens, keep_token_order)\n",
    "    shrink_model_embeddings(model, keep_indices, mapping=mapping)\n",
    "    return mapping\n",
    "\n",
    "def fix_dtypes(model, fix_weights=True, fix_quant_states=True):\n",
    "    \"\"\"\n",
    "    모델의 데이터 타입을 수정하는 함수\n",
    "    \n",
    "    Args:\n",
    "        model: 언어 모델\n",
    "        fix_weights: 가중치 타입 수정 여부\n",
    "        fix_quant_states: 양자화 상태 타입 수정 여부\n",
    "    \n",
    "    Returns:\n",
    "        수정된 모델\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    for module in model.modules():\n",
    "        weight = getattr(module, 'weight', None)\n",
    "        if weight is not None:\n",
    "            if torch.is_floating_point(weight):\n",
    "                # 부동소수점 가중치 타입 수정\n",
    "                if fix_weights and weight.dtype!=model.dtype:\n",
    "                    module.to(model.dtype)\n",
    "            else:\n",
    "                # 양자화된 가중치의 상태 타입 수정\n",
    "                qs = getattr(weight, 'quant_state', None)\n",
    "                if qs is not None:\n",
    "                    if fix_quant_states and qs.dtype!=model.dtype:\n",
    "                        qs.dtype = model.dtype\n",
    "    return model\n",
    "\n",
    "def merge_peft_into_base(model):\n",
    "    \"\"\"\n",
    "    PEFT(Parameter Efficient Fine-Tuning) 모델을 베이스 모델에 병합하는 함수\n",
    "    \n",
    "    Args:\n",
    "        model: PEFT 모델\n",
    "    \n",
    "    Returns:\n",
    "        병합된 베이스 모델\n",
    "    \"\"\"\n",
    "    print('*** PEFT 모델을 베이스 모델에 병합 중...')\n",
    "    assert is_peft_model(model)\n",
    "    return fix_dtypes(model.merge_and_unload())\n",
    "\n",
    "def save_model(store_path, model=None, tokenizer=None, merge=False):\n",
    "    \"\"\"\n",
    "    모델과 토크나이저를 저장하는 함수\n",
    "    \n",
    "    Args:\n",
    "        store_path: 저장 경로\n",
    "        model: 저장할 모델 (선택사항)\n",
    "        tokenizer: 저장할 토크나이저 (선택사항)\n",
    "        merge: PEFT 모델 병합 여부\n",
    "    \n",
    "    Returns:\n",
    "        처리된 모델\n",
    "    \"\"\"\n",
    "    if merge: model = merge_peft_into_base(model)\n",
    "    if store_path is not None:\n",
    "        assert model is not None or tokenizer is not None\n",
    "        print(f\"*** {'병합된 ' if merge else ''}모델/토크나이저를 '{store_path}'에 저장 중...\")\n",
    "        if model is not None: model.save_pretrained(store_path)\n",
    "        if tokenizer is not None:\n",
    "            tokenizer.save_pretrained(store_path)\n",
    "            # 불필요한 tokenizer.model 파일 삭제\n",
    "            to_delete = os.path.join(store_path, 'tokenizer.model')\n",
    "            if os.path.isfile(to_delete): os.remove(to_delete)\n",
    "    return model\n",
    "\n",
    "def is_unsloth_model(model):\n",
    "    \"\"\"Unsloth 모델인지 확인하는 함수\"\"\"\n",
    "    return model.model_tags is not None and 'unsloth' in model.model_tags\n",
    "\n",
    "def is_peft_model(model):\n",
    "    \"\"\"PEFT 모델인지 확인하는 함수\"\"\"\n",
    "    return hasattr(model, 'peft_type')\n",
    "\n",
    "def download_model(repo_id, store_path, get_name=lambda n: os.path.join(n.replace('/', '--'), 'transformers', 'default', '1')):\n",
    "    \"\"\"\n",
    "    HuggingFace에서 모델을 다운로드하는 함수\n",
    "    \n",
    "    Args:\n",
    "        repo_id: HuggingFace 모델 저장소 ID\n",
    "        store_path: 로컬 저장 경로\n",
    "        get_name: 파일명 생성 함수\n",
    "    \n",
    "    Returns:\n",
    "        모델 경로\n",
    "    \"\"\"\n",
    "    import os\n",
    "    if os.path.exists(repo_id): return repo_id\n",
    "    model_path = os.path.join(store_path, get_name(repo_id))\n",
    "    if not os.path.exists(model_path):\n",
    "        from huggingface_hub import snapshot_download\n",
    "        download_path = snapshot_download(repo_id=repo_id)\n",
    "        os.makedirs(os.path.split(model_path)[0], exist_ok=True)\n",
    "        os.symlink(download_path, model_path, target_is_directory=True)\n",
    "    return model_path\n",
    "\n",
    "def get_and_fix_peft_weights(store):\n",
    "    \"\"\"\n",
    "    PEFT 가중치를 로드하고 수정하는 함수\n",
    "    \n",
    "    Args:\n",
    "        store: PEFT 가중치 저장 경로\n",
    "    \n",
    "    Returns:\n",
    "        수정된 state_dict\n",
    "    \"\"\"\n",
    "    print(f\"*** '{store}'에서 PEFT state_dict 로드 중...\")\n",
    "    from peft import load_peft_weights\n",
    "    state_dict = load_peft_weights(store)\n",
    "    # 불필요한 modules_to_save 관련 키들 제거\n",
    "    for k in list(state_dict.keys()):\n",
    "        if 'modules_to_save' in k:\n",
    "            del state_dict[k]\n",
    "            original_module_key = k.replace('.modules_to_save.', '.original_module.')\n",
    "            if original_module_key in state_dict: del state_dict[original_module_key]\n",
    "            assert k.replace('.modules_to_save.', '.') in state_dict\n",
    "    return state_dict\n",
    "\n",
    "def set_peft_weights(model, state_dict):\n",
    "    \"\"\"\n",
    "    모델에 PEFT 가중치를 설정하는 함수\n",
    "    \n",
    "    Args:\n",
    "        model: 타겟 모델\n",
    "        state_dict: PEFT 가중치 딕셔너리\n",
    "    \"\"\"\n",
    "    print(f\"*** 모델 state_dict 설정 중...\")\n",
    "    from peft import set_peft_model_state_dict\n",
    "    res = set_peft_model_state_dict(model, state_dict)\n",
    "    assert not res.unexpected_keys\n",
    "\n",
    "def load_peft_state(model, store):\n",
    "    \"\"\"\n",
    "    저장된 PEFT 상태를 모델에 로드하는 함수\n",
    "    \n",
    "    Args:\n",
    "        model: 타겟 모델\n",
    "        store: PEFT 상태 저장 경로\n",
    "    \"\"\"\n",
    "    set_peft_weights(model, get_and_fix_peft_weights(store))\n",
    "\n",
    "def prepare_model(model, mode, tokenizer=None, formatter=None, shrink_embedding=False, dequantize=False, peft=[], local_files_only=False, add_special_tokens={}, set_pad_token=None, keep_tokens=[], keep_normalizer=None, peft_trainable=True, device_map=None, tf_grad_cp=True, tf_use_fa2=True, **kwargs):\n",
    "    \"\"\"\n",
    "    모델과 토크나이저를 준비하는 메인 함수\n",
    "    \n",
    "    Args:\n",
    "        model: 모델 경로 또는 모델 객체\n",
    "        mode: 로드 모드 ('unsloth_4bit', 'transformers', 'transformers_bf16', 등)\n",
    "        tokenizer: 토크나이저 (선택사항)\n",
    "        formatter: 데이터 포맷터 (선택사항)\n",
    "        shrink_embedding: 임베딩 축소 여부\n",
    "        dequantize: 역양자화 여부\n",
    "        peft: PEFT 설정 리스트\n",
    "        local_files_only: 로컬 파일만 사용 여부\n",
    "        add_special_tokens: 추가할 특별 토큰들\n",
    "        set_pad_token: 패딩 토큰 설정\n",
    "        keep_tokens: 유지할 토큰들\n",
    "        keep_normalizer: 정규화기 유지 여부\n",
    "        peft_trainable: PEFT 훈련 가능 여부\n",
    "        device_map: 디바이스 매핑\n",
    "        tf_grad_cp: 그래디언트 체크포인팅 사용 여부\n",
    "        tf_use_fa2: Flash Attention 2 사용 여부\n",
    "    \n",
    "    Returns:\n",
    "        (모델, 토크나이저, 포맷터) 튜플\n",
    "    \"\"\"\n",
    "    if isinstance(model, str):\n",
    "        assert tokenizer is None\n",
    "        print(f\"*** '{model}'에서 베이스 모델과 토크나이저 로드 중...\")\n",
    "        \n",
    "        if mode=='unsloth_4bit':\n",
    "            assert device_map is None, '지원되지 않음'\n",
    "            from unsloth import FastLanguageModel\n",
    "            model, tokenizer = FastLanguageModel.from_pretrained(model_name=model, dtype=None, load_in_4bit=True, local_files_only=local_files_only, **kwargs)\n",
    "        \n",
    "        elif mode in ['transformers', 'transformers_bf16', 'transformers_4bit', 'transformers_bf16_4bit', 'tokenizer_only']:\n",
    "            import torch\n",
    "            model_load_args = {}\n",
    "            if device_map is not None: model_load_args['device_map'] = device_map\n",
    "            if tf_use_fa2: model_load_args['attn_implementation'] = 'flash_attention_2'\n",
    "            if mode in ['transformers_bf16', 'transformers_bf16_4bit']: model_load_args['torch_dtype'] = torch.bfloat16\n",
    "            elif mode in ['transformers_4bit', 'transformers_bf16_4bit']:\n",
    "                from transformers import BitsAndBytesConfig\n",
    "                nf4_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type='nf4', bnb_4bit_use_double_quant=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "                model_load_args['quantization_config'] = nf4_config\n",
    "            \n",
    "            from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model, local_files_only=local_files_only, **kwargs)\n",
    "            model = AutoModelForCausalLM.from_pretrained(model, **model_load_args) if mode!='tokenizer_only' else None\n",
    "            if tf_grad_cp and model is not None: model.gradient_checkpointing_enable()\n",
    "        else: \n",
    "            raise NotImplementedError('알 수 없는 모드입니다.')\n",
    "    \n",
    "    # 특별 토큰 추가\n",
    "    if add_special_tokens: tokenizer.add_special_tokens(add_special_tokens)\n",
    "    if set_pad_token is not None: tokenizer.pad_token = set_pad_token\n",
    "    \n",
    "    # 포맷터 초기화\n",
    "    if formatter is not None and not hasattr(formatter, 'corpus'):\n",
    "        formatter = formatter(tokenizer=tokenizer)\n",
    "    \n",
    "    # 임베딩 축소\n",
    "    if (shrink_embedding<len(tokenizer.vocab) if type(shrink_embedding)==int else shrink_embedding) or keep_normalizer is False:\n",
    "        print('*** 임베딩 축소 중...')\n",
    "        embedding_size_before_shrink = len(tokenizer.vocab)\n",
    "        mapping = shrink_embeddings(model, tokenizer, formatter.get_corpus(), keep_tokens=keep_tokens, keep_normalizer=keep_normalizer)\n",
    "        print(f'*** -> 임베딩 크기를 {embedding_size_before_shrink}에서 {len(mapping)} 단어로 축소했습니다.')\n",
    "    \n",
    "    # 역양자화\n",
    "    if dequantize:\n",
    "        print(f'*** 모델 역양자화 중...')\n",
    "        model = model.dequantize()\n",
    "    \n",
    "    # PEFT 설정\n",
    "    if len(peft):\n",
    "        peft_trained = True if is_peft_model(model) else None\n",
    "        for i, m in enumerate(peft):\n",
    "            if peft_trained is True: model, peft_trained = merge_peft_into_base(model), None\n",
    "            if isinstance(m, str):\n",
    "                if peft_trained is False:\n",
    "                    _, peft_trained = load_peft_state(model, m), True\n",
    "                else:\n",
    "                    print(f\"*** '{m}'에서 PEFT 모델 로드 중...\")\n",
    "                    # unsloth 사용 시 주의: PeftModel로 로드하면 unsloth 최적화가 적용되지 않음\n",
    "                    from peft import PeftModel\n",
    "                    model, peft_trained = PeftModel.from_pretrained(model, m, trainable=peft_trainable), True\n",
    "            else:\n",
    "                assert peft_trained is None\n",
    "                if isinstance(m, dict):\n",
    "                    print('*** 새 PEFT 모델 생성 중...')\n",
    "                    if is_unsloth_model(model):\n",
    "                        from unsloth import FastLanguageModel\n",
    "                        my_get_peft_model = FastLanguageModel.get_peft_model\n",
    "                    else:\n",
    "                        from peft import LoraConfig, get_peft_model\n",
    "                        my_get_peft_model = lambda model, **kwargs: get_peft_model(model, LoraConfig(**kwargs))\n",
    "                    model, peft_trained = my_get_peft_model(model, **m), False\n",
    "                else: assert m is None\n",
    "    \n",
    "    return model, tokenizer, formatter\n",
    "\n",
    "def training_run(model, formatter, dataset, train_args, max_seq_length, merge=False, store=None, packing=False, grad_acc_fix=False, optimizers=None):\n",
    "    \"\"\"\n",
    "    모델 훈련을 실행하는 함수\n",
    "    \n",
    "    Args:\n",
    "        model: 훈련할 모델\n",
    "        formatter: 데이터 포맷터\n",
    "        dataset: 훈련 데이터셋\n",
    "        train_args: 훈련 인자들\n",
    "        max_seq_length: 최대 시퀀스 길이\n",
    "        merge: 훈련 후 병합 여부\n",
    "        store: 모델 저장 경로\n",
    "        packing: 시퀀스 패킹 사용 여부\n",
    "        grad_acc_fix: 그래디언트 누적 수정 여부\n",
    "        optimizers: 최적화 도구\n",
    "    \n",
    "    Returns:\n",
    "        (모델, 훈련 통계) 튜플\n",
    "    \"\"\"\n",
    "    assert merge is False, \"훈련 후 병합은 작동하지 않는 것으로 보임 (적어도 unsloth에서는 저장된 병합 모델이 훈련되지 않은 가중치를 포함함!)\"\n",
    "    import torch\n",
    "    from datasets import Dataset\n",
    "    add_train_args = {}\n",
    "    \n",
    "    # Unsloth 또는 일반 Transformers 설정\n",
    "    if is_unsloth_model(model):\n",
    "        from unsloth import FastLanguageModel\n",
    "        from unsloth import UnslothTrainer as Trainer\n",
    "        from unsloth import UnslothTrainingArguments as TrainingArguments\n",
    "        from unsloth import is_bfloat16_supported\n",
    "        FastLanguageModel.for_training(model)\n",
    "        add_train_args.update(fp16=not is_bfloat16_supported(), bf16=is_bfloat16_supported())\n",
    "    else:\n",
    "        from trl import SFTConfig as TrainingArguments\n",
    "        from trl import SFTTrainer as Trainer\n",
    "        model.train()\n",
    "        add_train_args.update(bf16=True)\n",
    "\n",
    "    # 토크나이저 설정\n",
    "    formatter.tokenizer.padding_side = 'right'\n",
    "    \n",
    "    # Unsloth 모델의 임베딩을 float32로 변환\n",
    "    if is_unsloth_model(model):\n",
    "        for convert_to_float in [model.get_input_embeddings(), model.get_output_embeddings()]:\n",
    "            if convert_to_float.weight.dtype!=torch.float32: convert_to_float.to(torch.float32)\n",
    "\n",
    "    add_args = {}\n",
    "    if optimizers is not None: add_args['optimizers'] = optimizers\n",
    "\n",
    "    # 트레이너 설정\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=formatter.tokenizer,\n",
    "        data_collator=formatter.get_data_collator(),\n",
    "        train_dataset=Dataset.from_list(dataset.as_list(formatter)),\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=max_seq_length,\n",
    "        dataset_num_proc=None,\n",
    "        packing=packing,  # 짧은 시퀀스에 대해 훈련 속도를 5배 향상시킬 수 있음\n",
    "        **add_args,\n",
    "        args=TrainingArguments(\n",
    "            **add_train_args,\n",
    "            **train_args\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    print('*** 훈련 실행 시작...')\n",
    "    # 그래디언트 누적 수정 사용 여부에 따른 훈련 실행\n",
    "    if grad_acc_fix and is_unsloth_model(model):\n",
    "        from unsloth import unsloth_train\n",
    "        trainer_stats = unsloth_train(trainer)\n",
    "    else:\n",
    "        if is_unsloth_model(model) and train_args['gradient_accumulation_steps']>1: \n",
    "            print('*** 경고: 결함이 있는 unsloth 그래디언트 누적을 사용 중')\n",
    "        trainer_stats = trainer.train()\n",
    "    \n",
    "    try: print(f'*** -> 훈련이 {trainer_stats.metrics[\"train_runtime\"]}초 소요되었습니다.')\n",
    "    except: pass\n",
    "    \n",
    "    if store is not None: save_model(store, model, formatter.tokenizer, merge=merge)\n",
    "    return model, trainer_stats\n",
    "\n",
    "def inference_load(store, keys=True, result_dict=None, always_read_from_file=False):\n",
    "    \"\"\"\n",
    "    저장된 추론 결과를 로드하는 함수\n",
    "    \n",
    "    Args:\n",
    "        store: 결과 저장 경로\n",
    "        keys: 로드할 키들 (True면 모든 키)\n",
    "        result_dict: 결과를 저장할 딕셔너리\n",
    "        always_read_from_file: 항상 파일에서 읽을지 여부\n",
    "    \n",
    "    Returns:\n",
    "        로드된 결과 딕셔너리\n",
    "    \"\"\"\n",
    "    if result_dict is None: result_dict = {}\n",
    "    if store is not None:\n",
    "        if keys is True: keys = os.listdir(store)\n",
    "        for key in keys:\n",
    "            if always_read_from_file or key not in result_dict:\n",
    "                try:\n",
    "                    with bz2.BZ2File(os.path.join(store, key)) as f: \n",
    "                        result_dict[key] = pickle.load(f)\n",
    "                except: continue\n",
    "    return result_dict\n",
    "\n",
    "def inference_save(store, key, outputs):\n",
    "    \"\"\"\n",
    "    추론 결과를 저장하는 함수\n",
    "    \n",
    "    Args:\n",
    "        store: 저장 경로\n",
    "        key: 결과 키\n",
    "        outputs: 저장할 출력 결과\n",
    "    \"\"\"\n",
    "    if store is not None:\n",
    "        os.makedirs(store, exist_ok=True)\n",
    "        with bz2.BZ2File(os.path.join(store, key), 'w') as f: \n",
    "            pickle.dump(outputs, f)\n",
    "\n",
    "class Decoder(object):\n",
    "    \"\"\"\n",
    "    모델 추론 결과를 디코딩하고 평가하는 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self, formatter, dataset, n_guesses, max_outputs=None, frac_score=False, quiet=False, name='', additional_decoders=None, prob_baseline=None):\n",
    "        \"\"\"\n",
    "        Decoder 초기화\n",
    "        \n",
    "        Args:\n",
    "            formatter: 데이터 포맷터\n",
    "            dataset: 평가 데이터셋\n",
    "            n_guesses: 허용되는 최대 추측 횟수\n",
    "            max_outputs: 최대 출력 개수\n",
    "            frac_score: 분수 점수 사용 여부\n",
    "            quiet: 조용한 모드 여부\n",
    "            name: 디코더 이름\n",
    "            additional_decoders: 추가 디코더들\n",
    "            prob_baseline: 확률 기준선\n",
    "        \"\"\"\n",
    "        self.formatter = formatter\n",
    "        self.dataset = dataset\n",
    "        self.n_guesses = n_guesses\n",
    "        self.decoded_results = {}  # 디코딩된 결과들\n",
    "        self.correct_solutions = {}  # 정답 솔루션들\n",
    "        self.keys_lim = set()  # 제한된 추측 내에서 정답을 맞춘 키들\n",
    "        self.keys_all = set()  # 모든 추측에서 정답을 맞춘 키들\n",
    "        self.mult_cnt = {}  # 배수 카운트\n",
    "        self.keys_cnt = {}  # 키 카운트\n",
    "        self.frac_score = frac_score\n",
    "        self.max_outputs = max_outputs\n",
    "        self.quiet = quiet\n",
    "        # 입력과 응답의 길이 정보 계산\n",
    "        self.input_len = [{} if formatter is not None and formatter.tokenizer is None else ds.get_lengths(formatter, name='input') for ds in [dataset, dataset.mod(np.transpose, keep_key=True)]]\n",
    "        self.reply_len = [{} if formatter is not None and formatter.tokenizer is None else ds.get_lengths(formatter, name='reply') for ds in [dataset, dataset.mod(np.transpose, keep_key=True)]]\n",
    "        self.additional_decoders = additional_decoders\n",
    "        self.name = name\n",
    "        self.prob_tracker = {}  # 확률 추적기\n",
    "        self.prob_tracker_best = {}  # 최고 확률 추적기\n",
    "        self.prob_baseline = prob_baseline\n",
    "\n",
    "    def score(self, *to_score):\n",
    "        \"\"\"\n",
    "        점수를 계산하는 함수\n",
    "        \n",
    "        Args:\n",
    "            *to_score: 점수를 매길 집합들\n",
    "        \n",
    "        Returns:\n",
    "            (점수들, 총 개수) 튜플\n",
    "        \"\"\"\n",
    "        scores = [(sum(1/self.mult_cnt[k.split('_')[0]] for k in s) if self.frac_score else len(s)) for s in to_score]\n",
    "        score_cnt = len(self.mult_cnt if self.frac_score else self.keys_cnt)\n",
    "        return scores, score_cnt\n",
    "\n",
    "    def from_store(self, store, **kwargs):\n",
    "        \"\"\"\n",
    "        저장소에서 결과를 로드하여 처리하는 함수\n",
    "        \n",
    "        Args:\n",
    "            store: 저장소 경로\n",
    "            **kwargs: 추가 인자들\n",
    "        \n",
    "        Returns:\n",
    "            self 객체\n",
    "        \"\"\"\n",
    "        for key, outputs in inference_load(store).items():\n",
    "            self.process(key, outputs, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def score_fmt(self, v):\n",
    "        \"\"\"점수 포맷팅 함수\"\"\"\n",
    "        return f'{v:5.1f}' if self.frac_score else f'{v:3}'\n",
    "\n",
    "    def process_single_output(self, key, output_len, decoded, print_func=print, len_info=None, device_info=None):\n",
    "        \"\"\"\n",
    "        단일 출력을 처리하는 함수\n",
    "        \n",
    "        Args:\n",
    "            key: 결과 키\n",
    "            output_len: 출력 길이\n",
    "            decoded: 디코딩된 결과\n",
    "            print_func: 출력 함수\n",
    "            len_info: 길이 정보\n",
    "            device_info: 디바이스 정보\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        # 데이터셋 변환 역변환 적용\n",
    "        inv_mod = {k: v if k.endswith('val') else self.dataset.invert_mod(v, key, inv_perm=(k.startswith('output') or k.startswith('score_all'))) for k, v in decoded.items()}\n",
    "        base_key = key.split('.')[0]\n",
    "        self.decoded_results[base_key] = self.decoded_results.get(base_key, {})\n",
    "        self.decoded_results[base_key][key] = inv_mod\n",
    "        output = inv_mod.get('output')\n",
    "        score = inv_mod.get('score')\n",
    "\n",
    "        # 빠른 점수 계산\n",
    "        self.keys_cnt[base_key] = self.keys_cnt.get(base_key, 0) + 1\n",
    "        mult_key, mult_sub = (base_key.split('_') + ['0'])[:2]\n",
    "        self.mult_cnt[mult_key] = max(self.mult_cnt.get(mult_key, 0), int(mult_sub) + 1)\n",
    "        \n",
    "        if len(self.dataset.replies):\n",
    "            correct_solution = self.dataset.replies.get(base_key)\n",
    "            if correct_solution is not None:\n",
    "                correct_solution = correct_solution[0]\n",
    "                self.correct_solutions[base_key] = correct_solution\n",
    "                is_correct = correct_solution is not None and np.array_equal(correct_solution, output)\n",
    "                if is_correct:\n",
    "                    self.keys_all.add(base_key)\n",
    "                    if self.keys_cnt[base_key] <= self.n_guesses: self.keys_lim.add(base_key)\n",
    "            \n",
    "            # 정답 여부 문자열 생성\n",
    "            corr_str = 'cant_decode' if output is None else 'sol_unknown' if correct_solution is None else 'ALL_CORRECT' if is_correct else 'bad_xy_size' if np.shape(correct_solution)!=np.shape(output) else 'bad_content'\n",
    "            (score_lim, score_all), score_cnt = self.score(self.keys_lim, self.keys_all)\n",
    "\n",
    "            tp_arr = (key.count('transpose') + key.count('rot90')) % 2\n",
    "            msc = None if score is None else np.sum(score)\n",
    "            fsc = inv_mod.get('score_val')\n",
    "            \n",
    "            # 확률 추적\n",
    "            if output is not None and fsc is not None:\n",
    "                pt = self.prob_tracker[base_key] = self.prob_tracker.get(base_key, {})\n",
    "                hash = tuple(map(tuple, output))\n",
    "                prob = pt[hash] = pt.get(hash, 0) + (np.exp(fsc) if self.prob_baseline is None else fsc - np.log(self.prob_baseline))\n",
    "                current_best = self.prob_tracker_best.get(base_key)\n",
    "                if current_best is None or current_best[0]<prob:\n",
    "                    self.prob_tracker_best[base_key] = (prob, output)\n",
    "            \n",
    "            # 결과 출력 포맷팅\n",
    "            fmt_name = f'{self.name}: ' if self.name else ''\n",
    "            msc_print = f'{min(-msc, 9.99999):7.5f}' if msc is not None else 'unknown'\n",
    "            fsc_print = f'{min(-fsc, 9.99999):7.5f}' if fsc is not None else 'unknown'\n",
    "            if not self.quiet: \n",
    "                print_func(f\" {fmt_name}acc: {self.score_fmt(score_lim)}/{score_cnt:3}={min(score_lim/score_cnt, 0.999):5.1%} (2-guess), {self.score_fmt(score_all)}/{score_cnt:3}={min(score_all/score_cnt, 0.999):5.1%} (any);{f' {device_info}' if device_info else ''} tok:{self.input_len[tp_arr].get(base_key, '?'):>4}+{self.reply_len[tp_arr].get(base_key, '?'):>3}>{'n/a' if output_len is None else output_len:>3} {corr_str}:{msc_print}|{fsc_print} [{key}]\")\n",
    "\n",
    "    def get_current_best(self, base_key):\n",
    "        \"\"\"\n",
    "        현재 최고 결과를 가져오는 함수\n",
    "        \n",
    "        Args:\n",
    "            base_key: 기본 키\n",
    "        \n",
    "        Returns:\n",
    "            최고 결과 또는 None\n",
    "        \"\"\"\n",
    "        current_best = self.prob_tracker_best.get(base_key)\n",
    "        return None if current_best is None else current_best[1]\n",
    "\n",
    "    def process_single_decode(self, key, de_tokenized, print_func=print, **kwargs):\n",
    "        \"\"\"\n",
    "        단일 디코딩 결과를 처리하는 함수\n",
    "        \n",
    "        Args:\n",
    "            key: 결과 키\n",
    "            de_tokenized: 디토크나이즈된 결과\n",
    "            print_func: 출력 함수\n",
    "            **kwargs: 추가 인자들\n",
    "        \"\"\"\n",
    "        # 호환성을 위한 포맷 확인\n",
    "        if len(de_tokenized)==3 and not isinstance(de_tokenized[1], float):  \n",
    "            output_len, *data = de_tokenized\n",
    "            score_val = None\n",
    "        else: \n",
    "            output_len, score_val, *data = de_tokenized\n",
    "        \n",
    "        if self.formatter is None:\n",
    "            assert len(data) == 1\n",
    "            decoded = [data[0]]\n",
    "        else: \n",
    "            decoded = self.formatter.decode_to_array(*data)\n",
    "        \n",
    "        # 점수 값 추가\n",
    "        for d in decoded: d['score_val'] = score_val\n",
    "        \n",
    "        # 각 디코딩 결과 처리\n",
    "        for i, dec in enumerate(decoded):\n",
    "            if i==0: \n",
    "                self.process_single_output(key, output_len, dec, print_func=print_func, **kwargs)\n",
    "            elif self.additional_decoders:\n",
    "                if i-1<len(self.additional_decoders): \n",
    "                    self.additional_decoders[i-1].process_single_output(key, output_len, dec, print_func=print_func, **kwargs)\n",
    "                else: \n",
    "                    print_func(f'{key} 출력 #{i}에 사용할 수 있는 디코더가 없습니다')\n",
    "            else: \n",
    "                self.process_single_output(f'{key}.fix{i}', output_len, dec, print_func=print_func, **kwargs)\n",
    "\n",
    "    def process(self, key, de_tokenized, **kwargs):\n",
    "        \"\"\"\n",
    "        디토크나이즈된 결과들을 처리하는 함수\n",
    "        \n",
    "        Args:\n",
    "            key: 결과 키\n",
    "            de_tokenized: 디토크나이즈된 결과들\n",
    "            **kwargs: 추가 인자들\n",
    "        \"\"\"\n",
    "        for i, d in enumerate(de_tokenized):\n",
    "            if self.max_outputs is None or i<=self.max_outputs:\n",
    "                self.process_single_decode(f'{key}.out{i}', d, **kwargs)\n",
    "\n",
    "    def get_unsolved_keys(self):\n",
    "        \"\"\"\n",
    "        아직 해결되지 않은 키들을 반환하는 함수\n",
    "        \n",
    "        Returns:\n",
    "            해결되지 않은 키들의 리스트\n",
    "        \"\"\"\n",
    "        unsolved = []\n",
    "        for base_key, reply in self.dataset.replies.items():\n",
    "            if not any(np.array_equal(reply[0], s.get('output')) for s in self.decoded_results.get(base_key, {}).values()):\n",
    "                unsolved.append(base_key)\n",
    "        return unsolved\n",
    "\n",
    "    def run_selection_algo(self, selection_algorithm):\n",
    "        \"\"\"\n",
    "        선택 알고리즘을 실행하는 함수\n",
    "        \n",
    "        Args:\n",
    "            selection_algorithm: 선택 알고리즘 함수\n",
    "        \n",
    "        Returns:\n",
    "            선택된 결과들의 딕셔너리\n",
    "        \"\"\"\n",
    "        return {bk: (selection_algorithm({k: g for k, g in v.items() if g.get('output') is not None}) if any(g.get('output') is not None for g in v.values()) else []) for bk, v in self.decoded_results.items()}\n",
    "\n",
    "    def benchmark_selection_algos(self, selection_algorithms, skip_failed=True):\n",
    "        \"\"\"\n",
    "        선택 알고리즘들을 벤치마크하는 함수\n",
    "        \n",
    "        Args:\n",
    "            selection_algorithms: 테스트할 선택 알고리즘들\n",
    "            skip_failed: 실패한 알고리즘 건너뛸지 여부\n",
    "        \n",
    "        Returns:\n",
    "            벤치마크 결과 딕셔너리\n",
    "        \"\"\"\n",
    "        import numpy as np\n",
    "        results = {}\n",
    "        print('*** 선택 알고리즘 벤치마크 중...')\n",
    "        for selection_algorithm in selection_algorithms:\n",
    "            name = selection_algorithm.__name__\n",
    "            try:\n",
    "                selected = self.run_selection_algo(selection_algorithm)\n",
    "                if self.formatter is not None:\n",
    "                    for sols in selected.values():\n",
    "                        for s in sols:\n",
    "                            assert self.formatter.is_valid_solution(s), f'유효하지 않은 솔루션 발견 {s}'\n",
    "                correct_keys = {k for k, v in selected.items() if self.correct_solutions.get(k) is not None and any(np.array_equal(guess, self.correct_solutions[k]) for guess in v[:self.n_guesses])}\n",
    "                (score,), score_cnt = self.score(correct_keys)\n",
    "                results[name] = score\n",
    "                print(f\" acc: {score:5.1f}/{score_cnt:3}={score/score_cnt:6.2%} ('{name}')\")\n",
    "            except:\n",
    "                print(f\" {'실행 실패':>21} ('{name}')\")\n",
    "                if not skip_failed: raise\n",
    "        return results\n",
    "\n",
    "    def calc_augmented_scores(self, model, base_keys=None, store=None, seed=0, max_len=None, make_unique=True, quiet=False, **kwargs):\n",
    "        \"\"\"\n",
    "        증강된 점수를 계산하는 함수\n",
    "        \n",
    "        Args:\n",
    "            model: 평가할 모델\n",
    "            base_keys: 기본 키들 (None이면 모든 키)\n",
    "            store: 저장 경로\n",
    "            seed: 랜덤 시드\n",
    "            max_len: 최대 길이\n",
    "            make_unique: 고유성 확보 여부\n",
    "            quiet: 조용한 모드 여부\n",
    "            **kwargs: 추가 인자들\n",
    "        \"\"\"\n",
    "        if base_keys is None: base_keys = list(self.decoded_results.keys())\n",
    "        if store is not None: store = f'{store}_new'  # 새 포맷은 하위 호환되지 않으므로 새 폴더 사용\n",
    "        \n",
    "        for bk in (base_keys if quiet else tqdm(base_keys, desc='증강된 점수 계산', file=sys.stdout)):\n",
    "            res = self.decoded_results.get(bk, {})\n",
    "            known_scores = {}\n",
    "            for k, v in sorted(res.items()):\n",
    "                if 'output' in v:\n",
    "                    k_store = None if store is None else os.path.join(store, k)\n",
    "                    id = tuple(map(tuple, v['output']))\n",
    "                    if not (make_unique and id in known_scores):\n",
    "                        try:\n",
    "                            assert k_store is not None\n",
    "                            with bz2.BZ2File(k_store) as f: \n",
    "                                known_scores[id] = pickle.load(f)\n",
    "                            # 하위 호환성을 위한 포맷 변환\n",
    "                            if isinstance(known_scores[id], list): \n",
    "                                known_scores[id] = dict(score_multi=known_scores[id])  \n",
    "                            k_store = None\n",
    "                        except:\n",
    "                            # 임시 데이터셋 생성하여 점수 계산\n",
    "                            temp_dataset = self.dataset.__class__(\n",
    "                                keys=[bk],\n",
    "                                queries={bk: self.dataset.queries.get(bk)},\n",
    "                                replies={bk: [v['output'].tolist()]},\n",
    "                            )\n",
    "                            temp_decoder = self.__class__(self.formatter, temp_dataset, n_guesses=self.n_guesses, quiet=True)\n",
    "                            temp_dataset = temp_dataset.augment(**kwargs, seed=(seed+hash(k)+hash(id)) % 1024**2, quiet=True)\n",
    "                            if max_len is not None: \n",
    "                                temp_dataset = temp_dataset.cut_to_len(formatter=self.formatter, name='input', max_len=max_len, quiet=True)\n",
    "                            for x in temp_dataset.as_list(self.formatter): \n",
    "                                calc_score(**x, formatter=self.formatter, model=model, decoder=temp_decoder)\n",
    "                            \n",
    "                            # 다양한 점수 메트릭 저장\n",
    "                            known_scores[id] = dict(\n",
    "                                score_multi=[np.sum(x['score']) for x in temp_decoder.decoded_results[bk].values()],\n",
    "                                score_multi_nl=[x['score_val'] for x in temp_decoder.decoded_results[bk].values()],\n",
    "                                score_multi_array=np.array([x['score'] for x in temp_decoder.decoded_results[bk].values()]),\n",
    "                                score_multi_array_cum=np.array([x['score_cum'] for x in temp_decoder.decoded_results[bk].values()]),\n",
    "                                score_multi_array_all=np.array([x['score_all'] for x in temp_decoder.decoded_results[bk].values()]),\n",
    "                                score_multi_array_all_cum=np.array([x['score_all_cum'] for x in temp_decoder.decoded_results[bk].values()]),\n",
    "                            )\n",
    "                            if k_store is not None:\n",
    "                                os.makedirs(store, exist_ok=True)\n",
    "                                with bz2.BZ2File(k_store, 'w') as f: \n",
    "                                    pickle.dump(known_scores[id], f)\n",
    "                    v.update(known_scores[id])\n",
    "\n",
    "def turbo_dfs(model, logits, path, eos_token_id, max_new_tokens, max_score, max_score_greedy, temperature, suppress_tokens, torch, score=0.0, pos=0, cache=None):\n",
    "    \"\"\"\n",
    "    터보 깊이 우선 탐색 함수 (효율적인 빔 서치 대안)\n",
    "    \n",
    "    Args:\n",
    "        model: 언어 모델\n",
    "        logits: 로짓 값들\n",
    "        path: 미리 계산된 경로\n",
    "        eos_token_id: 문장 종료 토큰 ID\n",
    "        max_new_tokens: 최대 새 토큰 수\n",
    "        max_score: 최대 점수\n",
    "        max_score_greedy: 탐욕적 최대 점수\n",
    "        temperature: 온도 파라미터\n",
    "        suppress_tokens: 억제할 토큰들\n",
    "        torch: PyTorch 모듈\n",
    "        score: 현재 점수\n",
    "        pos: 현재 위치\n",
    "        cache: 캐시\n",
    "    \n",
    "    Returns:\n",
    "        (점수, 접미사, 로짓들) 튜플들의 리스트\n",
    "    \"\"\"\n",
    "    logits, next_logits = logits[0], (logits[1:] if len(logits)>1 else None)\n",
    "    nll = -(logits / temperature).detach().float().log_softmax(-1).cpu().numpy()\n",
    "    greedy_index = nll.argmin(-1).item()\n",
    "    nll = list(enumerate(nll))\n",
    "    \n",
    "    # 미리 계산된 경로가 있으면 먼저 따라가기\n",
    "    if path: nll[0], nll[path[0]], path = nll[path[0]], nll[0], path[1:]  \n",
    "    \n",
    "    suffixes = []\n",
    "    for i, s in nll:\n",
    "        next_score = score + s\n",
    "        allowed_max_score = max_score_greedy if i==greedy_index else max_score\n",
    "        if next_score < allowed_max_score:\n",
    "            if i==eos_token_id: \n",
    "                next_suffixes = [(next_score, [], [])]\n",
    "            elif max_new_tokens>1:\n",
    "                if next_logits is None:\n",
    "                    # 캐시 크기 조정\n",
    "                    if pos<cache[0][0][0].shape[2]: \n",
    "                        cache[0] = tuple(tuple(c[:, :, :pos] for c in l) for l in cache[0])\n",
    "                    # 다음 토큰 생성\n",
    "                    next_logits, cache[0] = model(\n",
    "                        input_ids= torch.full((1,1), i, device=model.device),\n",
    "                        position_ids=torch.full((1,1), pos, device=model.device),\n",
    "                        past_key_values=cache[0],\n",
    "                    )[:2]\n",
    "                    next_logits = next_logits[0]  # 배치 차원 제거\n",
    "                # 재귀 호출\n",
    "                next_suffixes = turbo_dfs(model, logits=next_logits, path=path, eos_token_id=eos_token_id, max_new_tokens=max_new_tokens-1, max_score=max_score, max_score_greedy=allowed_max_score, temperature=temperature, suppress_tokens=suppress_tokens, torch=torch, score=next_score, pos=pos+1, cache=cache)\n",
    "            else: \n",
    "                next_suffixes = []\n",
    "            \n",
    "            # 접미사에 현재 토큰과 로짓 추가\n",
    "            for suffix in next_suffixes:\n",
    "                suffix[1].append(i)\n",
    "                suffix[2].append(logits)\n",
    "            suffixes.extend(next_suffixes)\n",
    "        next_logits = None\n",
    "    return suffixes\n",
    "\n",
    "def inference_turbo_dfs(model, input_ids, eos_token_id, max_new_tokens, min_prob, min_prob_greedy=1, temperature=1.0, suppress_tokens=[], path=[], attention_mask=None):\n",
    "    \"\"\"\n",
    "    터보 DFS를 사용한 추론 함수\n",
    "    \n",
    "    Args:\n",
    "        model: 언어 모델\n",
    "        input_ids: 입력 토큰 ID들\n",
    "        eos_token_id: 문장 종료 토큰 ID\n",
    "        max_new_tokens: 최대 새 토큰 수\n",
    "        min_prob: 최소 확률\n",
    "        min_prob_greedy: 탐욕적 최소 확률\n",
    "        temperature: 온도 파라미터\n",
    "        suppress_tokens: 억제할 토큰들\n",
    "        path: 경로\n",
    "        attention_mask: 어텐션 마스크\n",
    "    \n",
    "    Returns:\n",
    "        정렬된 결과 리스트 (점수, 접미사, 점수 배열)\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    with torch.no_grad():\n",
    "        assert attention_mask is None or attention_mask.all(), '구현되지 않음'\n",
    "        input_ids = torch.as_tensor(input_ids, device=model.device, dtype=int)\n",
    "        if input_ids.ndim==2: input_ids = input_ids.squeeze(0)\n",
    "        assert input_ids.ndim==1, '배칭은 지원되지 않음'\n",
    "        \n",
    "        # 점수 임계값 계산\n",
    "        max_score = -np.log(min_prob)\n",
    "        max_score_greedy = (-np.log(min_prob_greedy)) if min_prob_greedy>0 else float('inf')  \n",
    "        max_score_greedy = max(max_score, max_score_greedy)\n",
    "        \n",
    "        if path is None: path = []\n",
    "        if len(path) and path[-1]==eos_token_id: path = path[:-1]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            full_path = input_ids\n",
    "            if len(path): \n",
    "                full_path = torch.cat([full_path, torch.as_tensor(path, device=model.device)])\n",
    "            logits, cache = model(input_ids=full_path[np.newaxis])[:2]\n",
    "            logits = logits[0, len(input_ids)-1:]\n",
    "        \n",
    "        # 터보 DFS 실행\n",
    "        result = turbo_dfs(model, logits=logits, path=path, eos_token_id=eos_token_id, max_new_tokens=max_new_tokens, max_score=max_score, max_score_greedy=max_score_greedy, temperature=temperature, suppress_tokens=suppress_tokens, torch=torch, score=0.0, pos=len(input_ids), cache=[cache])\n",
    "        \n",
    "        # 결과 정렬하여 반환\n",
    "        return sorted([(score_val, np.array(suffix[::-1]), torch.stack(score_arr[::-1]).float().cpu().numpy()) for score_val, suffix, score_arr in result], key=lambda x:x[0])\n",
    "\n",
    "def inference_step(tokenized, model, remove_token_type_ids=True, num_beams=1, formatter=None, min_prob=None, current_best=None, **kwargs):\n",
    "    \"\"\"\n",
    "    추론 단계를 실행하는 함수\n",
    "    \n",
    "    Args:\n",
    "        tokenized: 토크나이즈된 입력\n",
    "        model: 언어 모델\n",
    "        remove_token_type_ids: 토큰 타입 ID 제거 여부\n",
    "        num_beams: 빔 개수\n",
    "        formatter: 포맷터\n",
    "        min_prob: 최소 확률\n",
    "        current_best: 현재 최고 결과\n",
    "        **kwargs: 추가 인자들\n",
    "    \n",
    "    Returns:\n",
    "        (토큰 출력, 점수 출력) 튜플\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    if remove_token_type_ids: tokenized.pop('token_type_ids', None)\n",
    "    \n",
    "    if min_prob is not None:\n",
    "        assert num_beams==1\n",
    "        # 터보 DFS 사용\n",
    "        gen = inference_turbo_dfs(model, **tokenized.to(model.device), path=current_best, min_prob=min_prob, eos_token_id=formatter.tokenizer.eos_token_id, **kwargs)\n",
    "        tokens_out = [[g[1] for g in gen]]\n",
    "        scores_out = [[g[2] for g in gen]]\n",
    "    elif is_unsloth_model(model) and num_beams > 1:\n",
    "        assert False, 'unsloth는 빔 서치를 지원하지 않습니다'\n",
    "    else:\n",
    "        # 표준 생성 방식\n",
    "        gen = model.generate(**tokenized.to(model.device), return_dict_in_generate=True, output_logits=True, use_cache=True, **kwargs)\n",
    "        tokens_out = gen['sequences'][:, torch.newaxis, tokenized['input_ids'].shape[-1]:].cpu().numpy().copy()\n",
    "        scores_out = torch.stack(gen['logits'], axis=-2)[:, torch.newaxis].float().cpu().numpy().copy()\n",
    "    return tokens_out, scores_out\n",
    "\n",
    "def process_inference_output(key, outputs, formatter, store=None, decoder=None, decoder_args={}):\n",
    "    \"\"\"\n",
    "    추론 출력을 처리하는 함수\n",
    "    \n",
    "    Args:\n",
    "        key: 결과 키\n",
    "        outputs: 출력 결과들\n",
    "        formatter: 포맷터\n",
    "        store: 저장 경로\n",
    "        decoder: 디코더\n",
    "        decoder_args: 디코더 인자들\n",
    "    \n",
    "    Returns:\n",
    "        디토크나이즈된 결과\n",
    "    \"\"\"\n",
    "    de_tokenized = [formatter.de_tokenize(*output) for output in zip(*outputs)]\n",
    "    inference_save(store, key, de_tokenized)\n",
    "    if decoder is not None: decoder.process(key, de_tokenized, **decoder_args)\n",
    "    return de_tokenized\n",
    "\n",
    "def inference_run_v2(model, formatter, dataset, decoder=None, max_new_tokens=None, max_batch_size=1, store=None, result_dict=None, rerun_empty=False, retrain=None, use_turbo=False, group_multi_output=True, **kwargs):\n",
    "    \"\"\"\n",
    "    추론 실행 함수 (버전 2)\n",
    "    \n",
    "    Args:\n",
    "        model: 언어 모델\n",
    "        formatter: 데이터 포맷터\n",
    "        dataset: 데이터셋\n",
    "        decoder: 디코더\n",
    "        max_new_tokens: 최대 새 토큰 수\n",
    "        max_batch_size: 최대 배치 크기\n",
    "        store: 저장 경로\n",
    "        result_dict: 결과 딕셔너리\n",
    "        rerun_empty: 빈 결과 재실행 여부\n",
    "        retrain: 재훈련 함수\n",
    "        use_turbo: 터보 모드 사용 여부\n",
    "        group_multi_output: 다중 출력 그룹화 여부\n",
    "        **kwargs: 추가 인자들\n",
    "    \n",
    "    Returns:\n",
    "        결과 딕셔너리\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    assert max_batch_size==1, '지원되지 않음'\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print('*** 저장된 데이터 로드 중...')\n",
    "        if result_dict is None: result_dict = {}\n",
    "        result_dict = inference_load(store, dataset.keys, result_dict)\n",
    "        \n",
    "        # 키들을 기본 키별로 그룹화\n",
    "        by_base_key = {}\n",
    "        needs_rerun = {}\n",
    "        base_key_list = []\n",
    "        for key in dataset.keys:\n",
    "            base_key = key.split('.')[0]\n",
    "            if group_multi_output: base_key = base_key.split('_')[0]\n",
    "            if base_key not in by_base_key: base_key_list.append(base_key)\n",
    "            bk_list = by_base_key[base_key] = by_base_key.get(base_key, [])\n",
    "            bk_list.append(key)\n",
    "        \n",
    "        # 재실행이 필요한 키들 찾기\n",
    "        for base_key, keys in by_base_key.items():\n",
    "            for key in keys:\n",
    "                de_tokenized = result_dict.get(key)\n",
    "                if de_tokenized is None or (rerun_empty and not de_tokenized):\n",
    "                    bk_list = needs_rerun[base_key] = needs_rerun.get(base_key, [])\n",
    "                    bk_list.append(key)\n",
    "                elif decoder is not None: \n",
    "                    decoder.process(key, de_tokenized)\n",
    "\n",
    "        # 모델을 추론 모드로 설정\n",
    "        formatter.tokenizer.padding_side = 'left'\n",
    "        if max_new_tokens is None: max_new_tokens = formatter.max_new_tokens()\n",
    "        if is_unsloth_model(model):\n",
    "            from unsloth import FastLanguageModel\n",
    "            FastLanguageModel.for_inference(model)\n",
    "        else: \n",
    "            model.eval()\n",
    "\n",
    "        print('*** 추론 실행 시작...')\n",
    "    try:\n",
    "        with tqdm(base_key_list, file=sys.stdout) as pbar:\n",
    "            for base_key in pbar:\n",
    "                run_keys = needs_rerun.get(base_key)\n",
    "                if run_keys:\n",
    "                    # 재훈련이 필요한 경우\n",
    "                    if retrain is not None:\n",
    "                        retrain_dataset = dataset.keep_key_startswith(base_key)\n",
    "                        print(f\"키 '{base_key}'에 대해 모델 재훈련 중 (retrain_dataset_size={len(retrain_dataset.keys)})\")\n",
    "                        retrain(model, retrain_dataset)\n",
    "                        if is_unsloth_model(model): FastLanguageModel.for_inference(model)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        for key in run_keys:\n",
    "                            # 입력 텍스트 준비\n",
    "                            input_text = dataset.get(key, formatter)['input']\n",
    "                            batch = formatter.tokenizer([input_text], return_tensors='pt')\n",
    "                            \n",
    "                            # 터보 모드에서 현재 최고 결과 사용\n",
    "                            current_best = decoder.get_current_best(key.split('.')[0]) if use_turbo else None\n",
    "                            if current_best is not None:\n",
    "                                current_best = dataset.forward_mod(current_best, key)\n",
    "                                current_best = formatter.fmt_reply([current_best])\n",
    "                                current_best = formatter.tokenizer(input_text+current_best)['input_ids'][batch['input_ids'].shape[-1]:]\n",
    "                            \n",
    "                            # 추론 실행\n",
    "                            batch_out = inference_step(batch, model, formatter=formatter, max_new_tokens=max_new_tokens, current_best=current_best, **kwargs)\n",
    "                            outputs = [x[0] for x in batch_out]\n",
    "                            result_dict[key] = process_inference_output(key, outputs, formatter, store=store, decoder=decoder, decoder_args=dict(print_func=pbar.write))\n",
    "        print('*** 추론 실행 완료.')\n",
    "    except KeyboardInterrupt: \n",
    "        print('*** Ctrl+C 눌림, 추론 실행 중단.')\n",
    "    return result_dict\n",
    "\n",
    "class Retrainer(object):\n",
    "    \"\"\"\n",
    "    모델 재훈련을 담당하는 클래스\n",
    "    \"\"\"\n",
    "    def __init__(self, n, aug_opts, reload_state_dict=None, **kwargs):\n",
    "        \"\"\"\n",
    "        Retrainer 초기화\n",
    "        \n",
    "        Args:\n",
    "            n: 훈련 샘플 수\n",
    "            aug_opts: 데이터 증강 옵션들\n",
    "            reload_state_dict: 재로드할 state_dict\n",
    "            **kwargs: 추가 인자들\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.aug_opts = aug_opts\n",
    "        self.reload_state_dict = reload_state_dict\n",
    "        self.kwargs = kwargs\n",
    "\n",
    "    def preprocess(self, dataset):\n",
    "        \"\"\"\n",
    "        데이터셋을 전처리하는 함수\n",
    "        \n",
    "        Args:\n",
    "            dataset: 입력 데이터셋\n",
    "        \n",
    "        Returns:\n",
    "            전처리된 데이터셋\n",
    "        \"\"\"\n",
    "        # 필요한 수만큼 데이터 증강\n",
    "        ds = [dataset.augment(quiet=True, shfl_keys=True, **self.aug_opts) for _ in range((self.n-1)//dataset.length()+1)]\n",
    "        ds = ds[0] if len(ds)==1 else ds[0].append(*ds[1:])\n",
    "        ds, _ = ds.split_at_pos(self.n)\n",
    "        return ds\n",
    "\n",
    "    def __call__(self, model, dataset):\n",
    "        \"\"\"\n",
    "        재훈련 실행\n",
    "        \n",
    "        Args:\n",
    "            model: 재훈련할 모델\n",
    "            dataset: 훈련 데이터셋\n",
    "        \"\"\"\n",
    "        if self.reload_state_dict is not None: \n",
    "            set_peft_weights(model, self.reload_state_dict)\n",
    "        \n",
    "        assert is_unsloth_model(model), '구현되지 않음'\n",
    "        if is_unsloth_model(model):\n",
    "            from unsloth import FastLanguageModel\n",
    "            FastLanguageModel.for_training(model)\n",
    "        else: \n",
    "            model.train()\n",
    "        \n",
    "        training_run(model, dataset=self.preprocess(dataset), **self.kwargs)\n",
    "\n",
    "def calc_score(key, input, reply, formatter, model, store=None, decoder=None, **_):\n",
    "    \"\"\"\n",
    "    주어진 입력-응답 쌍에 대한 점수를 계산하는 함수\n",
    "    \n",
    "    Args:\n",
    "        key: 데이터 키\n",
    "        input: 입력 텍스트\n",
    "        reply: 응답 텍스트\n",
    "        formatter: 데이터 포맷터\n",
    "        model: 언어 모델\n",
    "        store: 저장 경로\n",
    "        decoder: 디코더\n",
    "        **_: 무시되는 추가 인자들\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    with torch.no_grad():\n",
    "        # 입력 길이 계산\n",
    "        input_len = len(formatter.tokenizer(input)['input_ids'])\n",
    "        # 전체 시퀀스 토크나이즈\n",
    "        tokenized = formatter.tokenizer([input+reply], return_tensors='pt')\n",
    "        # 응답 부분의 토큰들만 추출\n",
    "        reply_tok = tokenized['input_ids'][0][input_len:].cpu().numpy().copy()\n",
    "        # 응답 부분의 로그 확률 계산\n",
    "        reply_log = model.forward(**tokenized.to(model.device))['logits'][0, input_len-1: -1].float().cpu().numpy().copy()\n",
    "        # 결과 처리\n",
    "        process_inference_output(key, (reply_tok[torch.newaxis], reply_log[torch.newaxis]), formatter, store=store, decoder=decoder)\n",
    "\n",
    "def mem_info(gpu_id=0):\n",
    "    \"\"\"\n",
    "    GPU 메모리 정보를 출력하는 함수\n",
    "    \n",
    "    Args:\n",
    "        gpu_id: GPU ID (기본값: 0)\n",
    "    \"\"\"\n",
    "    import torch\n",
    "    try:\n",
    "        gpu_stats = torch.cuda.get_device_properties(gpu_id)\n",
    "        usage = torch.cuda.max_memory_reserved() / 1024**3\n",
    "        avail = gpu_stats.total_memory / 1024**3\n",
    "        print(f\"*** GPU: {gpu_stats.name}, 사용량 {usage:.3} / {avail:.3} GB.\")\n",
    "    except: \n",
    "        print('*** 메모리 통계를 가져오는 중 예외가 발생했습니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6932e17",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T02:27:57.571823Z",
     "iopub.status.busy": "2025-07-07T02:27:57.571581Z",
     "iopub.status.idle": "2025-07-07T02:27:57.602994Z",
     "shell.execute_reply": "2025-07-07T02:27:57.602410Z"
    },
    "papermill": {
     "duration": 0.041212,
     "end_time": "2025-07-07T02:27:57.604468",
     "exception": false,
     "start_time": "2025-07-07T02:27:57.563256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile arc_loader.py\n",
    "import json\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import os, sys\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "def cut_at_token(output, token_id):\n",
    "    \"\"\"\n",
    "    특정 토큰에서 출력을 자르는 함수\n",
    "    \n",
    "    Args:\n",
    "        output: 출력 시퀀스\n",
    "        token_id: 자를 토큰 ID\n",
    "    \n",
    "    Returns:\n",
    "        자른 출력 시퀀스\n",
    "    \"\"\"\n",
    "    eos_positions = (output==token_id).nonzero()[0]\n",
    "    return output[:eos_positions[0]] if len(eos_positions) else output\n",
    "\n",
    "def shuffled(data_list):\n",
    "    \"\"\"데이터 리스트를 무작위로 섞는 함수\"\"\"\n",
    "    return np.random.permutation(data_list).tolist()\n",
    "\n",
    "def permute_mod(a, descriptor, invert=False):\n",
    "    \"\"\"\n",
    "    배열에 순열 변환을 적용하는 함수\n",
    "    \n",
    "    Args:\n",
    "        a: 변환할 배열\n",
    "        descriptor: 순열을 나타내는 문자열\n",
    "        invert: 역변환 여부\n",
    "    \n",
    "    Returns:\n",
    "        순열이 적용된 배열\n",
    "    \"\"\"\n",
    "    permutation = [int(i) for i in descriptor if str(i).isdigit()]\n",
    "    assert sorted(permutation)==list(range(10))\n",
    "    a = np.asarray(a)\n",
    "    if a.ndim==3:\n",
    "        # 3차원 배열의 경우 (색상 차원)\n",
    "        if not invert: permutation = np.argsort(permutation)\n",
    "        a = a[..., permutation]\n",
    "    else:\n",
    "        # 2차원 배열의 경우 (그리드)\n",
    "        assert a.ndim==2\n",
    "        if invert: permutation = np.argsort(permutation)\n",
    "        a = np.asarray(permutation)[a]\n",
    "    return a\n",
    "\n",
    "def permute_rnd_col_(query):\n",
    "    \"\"\"배경색(0)을 유지하고 나머지 색상을 무작위로 순열하는 함수\"\"\"\n",
    "    permutation = [0]+(1+np.random.permutation(9)).tolist()\n",
    "    return 'permute' + ''.join(map(str, permutation))\n",
    "\n",
    "def permute_rnd_all_(query):\n",
    "    \"\"\"모든 색상을 무작위로 순열하는 함수\"\"\"\n",
    "    permutation = np.random.permutation(10).tolist()\n",
    "    return 'permute' + ''.join(map(str, permutation))\n",
    "\n",
    "def permute_cnt_col_(query):\n",
    "    \"\"\"\n",
    "    배경색을 유지하고 나머지 색상을 빈도순으로 정렬하는 함수\n",
    "    (무작위성을 동점자 해결 기준으로 사용)\n",
    "    \"\"\"\n",
    "    elements, frequency = np.unique(np.concatenate([list(range(10))]+[np.array(x['input']).ravel() for x in query['train']]), return_counts=True)\n",
    "    permutation = [0]+sorted(np.random.permutation(9)+1, key=lambda i: frequency[i], reverse=True)  \n",
    "    return 'permute' + ''.join(map(str, permutation))\n",
    "\n",
    "def permute_cnt_all_(query):\n",
    "    \"\"\"모든 색상을 빈도순으로 정렬하는 함수 (무작위성을 동점자 해결 기준으로 사용)\"\"\"\n",
    "    elements, frequency = np.unique(np.concatenate([list(range(10))]+[np.array(x['input']).ravel() for x in query['train']]), return_counts=True)\n",
    "    permutation = sorted(np.random.permutation(10), key=lambda i: frequency[i], reverse=True)  \n",
    "    return 'permute' + ''.join(map(str, permutation))\n",
    "\n",
    "# 다양한 순열 변환 옵션들\n",
    "permute_rnd_col = (permute_mod, permute_rnd_col_)  # 배경색 유지, 무작위 순열\n",
    "permute_rnd_all = (permute_mod, permute_rnd_all_)  # 전체 무작위 순열\n",
    "permute_cnt_col = (permute_mod, permute_cnt_col_)  # 배경색 유지, 빈도순 정렬\n",
    "permute_cnt_all = (permute_mod, permute_cnt_all_)  # 전체 빈도순 정렬\n",
    "permute_None = (np.copy, None)  # 순열 없음 (복사만)\n",
    "\n",
    "class ArcDataset(object):\n",
    "    \"\"\"ARC 데이터셋을 처리하는 메인 클래스\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward_mod(a, key, use_perm=True, is_output=True):\n",
    "        \"\"\"\n",
    "        키에 따라 배열에 순방향 변환을 적용하는 함수\n",
    "        \n",
    "        Args:\n",
    "            a: 변환할 배열\n",
    "            key: 변환 정보가 포함된 키\n",
    "            use_perm: 순열 사용 여부\n",
    "            is_output: 출력 데이터 여부\n",
    "        \n",
    "        Returns:\n",
    "            변환된 배열\n",
    "        \"\"\"\n",
    "        if a is None: return a\n",
    "        for op in key.split('.')[1:]:\n",
    "            # 'I' 접두사는 입력에만 적용되는 변환을 의미\n",
    "            if op.startswith('I'):\n",
    "                if is_output: continue\n",
    "                op = op[1:]\n",
    "            # 다양한 변환 적용\n",
    "            if   op=='rot90':              a = np.rot90(a)  # 90도 회전\n",
    "            elif op=='transpose':          a = np.swapaxes(a, 0, 1)  # 전치\n",
    "            elif op.startswith('permute'): a = permute_mod(a, op, invert=False) if use_perm else a  # 순열\n",
    "            elif op.startswith('copy'):    a = np.copy(a)  # 복사\n",
    "            elif op.startswith('out'):     a = a  # 출력 표시\n",
    "            elif op.startswith('ex'):      a = a  # 예제 표시\n",
    "            elif op.startswith('fix'):     a = a  # 수정 표시\n",
    "            elif op.startswith('ice'):     a = a  # icecuber 솔루션 추가용\n",
    "            else: raise NotImplementedError(f\"연산 '{op}'의 역변환을 알 수 없습니다.\")\n",
    "        return a\n",
    "\n",
    "    @staticmethod\n",
    "    def invert_mod(a, key, inv_perm=True, is_output=True):\n",
    "        \"\"\"\n",
    "        키에 따라 배열에 역방향 변환을 적용하는 함수\n",
    "        \n",
    "        Args:\n",
    "            a: 변환할 배열\n",
    "            key: 변환 정보가 포함된 키\n",
    "            inv_perm: 순열 역변환 사용 여부\n",
    "            is_output: 출력 데이터 여부\n",
    "        \n",
    "        Returns:\n",
    "            역변환된 배열\n",
    "        \"\"\"\n",
    "        if a is None: return a\n",
    "        # 변환을 역순으로 적용\n",
    "        for op in key.split('.')[1:][::-1]:\n",
    "            if op.startswith('I'):\n",
    "                if is_output: continue\n",
    "                op = op[1:]\n",
    "            # 역변환 적용\n",
    "            if   op=='rot90':              a = np.rot90(np.rot90(np.rot90(a)))  # 270도 회전 (90도의 역)\n",
    "            elif op=='transpose':          a = np.swapaxes(a, 0, 1)  # 전치 (자기 자신이 역변환)\n",
    "            elif op.startswith('permute'): a = permute_mod(a, op, invert=True) if inv_perm else a\n",
    "            elif op.startswith('copy'):    a = np.copy(a)\n",
    "            elif op.startswith('out'):     a = a\n",
    "            elif op.startswith('ex'):      a = a\n",
    "            elif op.startswith('fix'):     a = a\n",
    "            elif op.startswith('ice'):     a = a\n",
    "            else: raise NotImplementedError(f\"연산 '{op}'의 역변환을 알 수 없습니다.\")\n",
    "        return a\n",
    "\n",
    "    def __init__(self, queries, replies={}, keys=None, is_orig=False, is_fake=False):\n",
    "        \"\"\"\n",
    "        ArcDataset 초기화\n",
    "        \n",
    "        Args:\n",
    "            queries: 문제 데이터 딕셔너리\n",
    "            replies: 답안 데이터 딕셔너리\n",
    "            keys: 사용할 키 리스트\n",
    "            is_orig: 원본 데이터셋 여부\n",
    "            is_fake: 가짜 테스트 세트 여부\n",
    "        \"\"\"\n",
    "        if keys is not None: keys = [k for k in keys if k is not None]\n",
    "        self.queries = queries if keys is None else {k: queries[k] for k in keys}\n",
    "        self.replies = replies if keys is None else {k: replies[k] for k in keys if k in replies}\n",
    "        self.is_orig = is_orig\n",
    "        self.is_fake = is_fake\n",
    "        self.keys = sorted(queries.keys()) if keys is None else keys\n",
    "        self.faulty = {}  # 결함이 있는 데이터 추적\n",
    "        self.transposed_dataset = None  # 전치된 데이터셋 캐시\n",
    "\n",
    "    @classmethod\n",
    "    def empty(cls):\n",
    "        \"\"\"빈 데이터셋을 생성하는 클래스 메서드\"\"\"\n",
    "        return cls(queries={}, replies={}, keys=[])\n",
    "\n",
    "    def change_keys(self, keys, keep_flags=False):\n",
    "        \"\"\"\n",
    "        키를 변경하여 새로운 데이터셋을 생성하는 함수\n",
    "        \n",
    "        Args:\n",
    "            keys: 새로운 키 리스트\n",
    "            keep_flags: 플래그 유지 여부\n",
    "        \n",
    "        Returns:\n",
    "            새로운 ArcDataset 인스턴스\n",
    "        \"\"\"\n",
    "        flags = dict(is_fake=self.is_fake, is_orig=self.is_orig) if keep_flags else {}\n",
    "        return self.__class__(queries=self.queries, replies=self.replies, keys=keys, **flags)\n",
    "\n",
    "    @classmethod\n",
    "    def from_file(cls, queries_file):\n",
    "        \"\"\"\n",
    "        파일에서 문제 데이터를 로드하는 클래스 메서드\n",
    "        \n",
    "        Args:\n",
    "            queries_file: 문제 파일 경로\n",
    "        \n",
    "        Returns:\n",
    "            로드된 ArcDataset 인스턴스\n",
    "        \"\"\"\n",
    "        print(f\"*** '{queries_file}'에서 문제 로드 중...\")\n",
    "        with open(queries_file) as f: queries = f.read()\n",
    "        # 가짜 테스트 세트 감지 (특정 MD5 해시로 판별)\n",
    "        is_fake = hashlib.md5(queries.encode('utf-8')).hexdigest().lower()=='a6b7dac3cab03abf2eb333e16610d6dc'\n",
    "        if is_fake: print(\"*** -> 가짜 테스트 세트 감지됨, 'is_fake' 플래그를 True로 설정.\")\n",
    "        return cls(\n",
    "            queries=json.loads(queries),\n",
    "            is_fake=is_fake,\n",
    "            is_orig=True,\n",
    "        )\n",
    "\n",
    "    def load_replies(self, replies_file):\n",
    "        \"\"\"\n",
    "        답안 파일을 로드하는 함수\n",
    "        \n",
    "        Args:\n",
    "            replies_file: 답안 파일 경로\n",
    "        \n",
    "        Returns:\n",
    "            self 객체\n",
    "        \"\"\"\n",
    "        print(f\"*** '{replies_file}'에서 솔루션 로드 중...\")\n",
    "        with open(replies_file) as f: replies = f.read()\n",
    "        replies_parsed = json.loads(replies)\n",
    "        self.replies = {k: replies_parsed[k] for k in self.keys}\n",
    "        return self\n",
    "\n",
    "    def split_multi_replies(self):\n",
    "        \"\"\"\n",
    "        다중 테스트 케이스를 개별 키로 분할하는 함수\n",
    "        \n",
    "        Returns:\n",
    "            분할된 새로운 ArcDataset\n",
    "        \"\"\"\n",
    "        key_indices = [(k, i) for k in self.keys for i in range(len(self.queries[k]['test']))]\n",
    "        return self.__class__(\n",
    "            keys=[f'{k}_{i}' for k, i in key_indices],\n",
    "            queries={f'{k}_{i}': {'train': self.queries[k]['train'], 'test': [self.queries[k]['test'][i]]} for k, i in key_indices},\n",
    "            replies={f'{k}_{i}': [self.replies[k][i]] for k, i in key_indices if k in self.replies},\n",
    "        )\n",
    "\n",
    "    def move_test_to_train(self):\n",
    "        \"\"\"\n",
    "        테스트 데이터를 훈련 데이터로 이동하는 함수\n",
    "        \n",
    "        Returns:\n",
    "            변환된 새로운 ArcDataset\n",
    "        \"\"\"\n",
    "        new_queries = {k: {'train': self.queries[k]['train'] + [{**t, 'output': self.replies[k][i]} for i, t in enumerate(self.queries[k]['test'])], 'test': []} for k in self.keys}\n",
    "        return self.__class__(queries=new_queries, keys=[k for k in self.keys])\n",
    "\n",
    "    def last_train_ex_for_test(self):\n",
    "        \"\"\"\n",
    "        마지막 훈련 예제를 테스트로 사용하는 함수\n",
    "        \n",
    "        Returns:\n",
    "            변환된 새로운 ArcDataset\n",
    "        \"\"\"\n",
    "        assert not self.replies\n",
    "        new_queries = {k: {'train': self.queries[k]['train'][:-1], 'test': [{'input': self.queries[k]['train'][-1]['input']}]} for k in self.keys}\n",
    "        new_replies = {k: [self.queries[k]['train'][-1]['output']] for k in self.keys}\n",
    "        return self.__class__(queries=new_queries, replies=new_replies, keys=[k for k in self.keys])\n",
    "\n",
    "    def length(self):\n",
    "        \"\"\"데이터셋의 길이를 반환하는 함수\"\"\"\n",
    "        return len(self.keys)\n",
    "\n",
    "    def shuffled(self, seed=None):\n",
    "        \"\"\"\n",
    "        키를 무작위로 섞은 새로운 데이터셋을 반환하는 함수\n",
    "        \n",
    "        Args:\n",
    "            seed: 랜덤 시드\n",
    "        \n",
    "        Returns:\n",
    "            섞인 새로운 ArcDataset\n",
    "        \"\"\"\n",
    "        if seed is not None: np.random.seed(seed)\n",
    "        return self.__class__(queries=self.queries, replies=self.replies, keys=shuffled(self.keys))\n",
    "\n",
    "    def sorted(self, **kwargs):\n",
    "        \"\"\"키를 정렬한 새로운 데이터셋을 반환하는 함수\"\"\"\n",
    "        return self.__class__(queries=self.queries, replies=self.replies, keys=sorted(self.keys, **kwargs))\n",
    "\n",
    "    def append(*datasets):\n",
    "        \"\"\"\n",
    "        여러 데이터셋을 결합하는 정적 메서드\n",
    "        \n",
    "        Args:\n",
    "            *datasets: 결합할 데이터셋들\n",
    "        \n",
    "        Returns:\n",
    "            결합된 새로운 ArcDataset\n",
    "        \"\"\"\n",
    "        return datasets[0].__class__(\n",
    "            queries={k: v for d in datasets for k, v in d.queries.items()},\n",
    "            replies={k: v for d in datasets for k, v in d.replies.items()},\n",
    "            keys   =[k    for d in datasets for k    in d.keys           ],\n",
    "        )\n",
    "\n",
    "    def sort_ex_by_input_size(self, seed=42, reverse=False):\n",
    "        \"\"\"\n",
    "        예제를 입력 크기순으로 정렬하는 함수\n",
    "        \n",
    "        Args:\n",
    "            seed: 랜덤 시드\n",
    "            reverse: 역순 정렬 여부\n",
    "        \n",
    "        Returns:\n",
    "            정렬된 새로운 ArcDataset\n",
    "        \"\"\"\n",
    "        np.random.seed(seed)\n",
    "        sort_key = lambda ex: np.prod(np.shape(ex['input']))\n",
    "        new_queries = {k2: {k: (sorted(np.random.permutation(np.array(v, dtype=object)), key=sort_key, reverse=reverse) if k=='train' else v) for k, v in v2.items()} for k2, v2 in self.queries.items()}\n",
    "        return self.__class__(queries=new_queries, replies=self.replies, keys=[k for k in self.keys])\n",
    "\n",
    "    def interleave(self, block_size, num_gpus=None):\n",
    "        \"\"\"\n",
    "        데이터를 인터리브하여 분산 처리를 위해 분할하는 함수\n",
    "        \n",
    "        Args:\n",
    "            block_size: 블록 크기\n",
    "            num_gpus: GPU 개수\n",
    "        \n",
    "        Returns:\n",
    "            인터리브된 데이터셋 또는 GPU별 데이터셋 리스트\n",
    "        \"\"\"\n",
    "        keys = np.reshape(self.keys, (-1, block_size)).T\n",
    "        if num_gpus is None: return self.change_keys(keys.ravel().tolist())\n",
    "        ret, num_gpus = (None, num_gpus) if isinstance(num_gpus, int) else num_gpus\n",
    "        keys = np.concatenate([keys, np.full((-keys.shape[0]%num_gpus, keys.shape[1]), None)])\n",
    "        keys = np.reshape(keys, (keys.shape[0]//num_gpus, num_gpus, -1)).swapaxes(0, 1).reshape(num_gpus, -1)\n",
    "        new_datasets = [self.change_keys(gpu_keys.tolist()) for gpu_keys in keys]\n",
    "        return new_datasets if ret is None else new_datasets[ret]\n",
    "\n",
    "    def remove(self, *datasets):\n",
    "        \"\"\"\n",
    "        지정된 데이터셋의 키들을 제거하는 함수\n",
    "        \n",
    "        Args:\n",
    "            *datasets: 제거할 데이터셋들\n",
    "        \n",
    "        Returns:\n",
    "            키가 제거된 새로운 ArcDataset\n",
    "        \"\"\"\n",
    "        remove_keys = {k for d in datasets for k in d.keys}\n",
    "        new_keys = [k for k in self.keys if k not in remove_keys]\n",
    "        return self.change_keys(new_keys)\n",
    "\n",
    "    def keep_key_startswith(self, key_start):\n",
    "        \"\"\"\n",
    "        특정 접두사로 시작하는 키만 유지하는 함수\n",
    "        \n",
    "        Args:\n",
    "            key_start: 키 접두사\n",
    "        \n",
    "        Returns:\n",
    "            필터링된 새로운 ArcDataset\n",
    "        \"\"\"\n",
    "        new_keys = [k for k in self.keys if k.startswith(key_start)]\n",
    "        return self.change_keys(new_keys)\n",
    "\n",
    "    def mod_single(self, mod_func, descriptor, i, keep_key, inputs_only):\n",
    "        \"\"\"\n",
    "        단일 변환을 적용하는 함수\n",
    "        \n",
    "        Args:\n",
    "            mod_func: 변환 함수\n",
    "            descriptor: 변환 설명자\n",
    "            i: 인덱스\n",
    "            keep_key: 키 유지 여부\n",
    "            inputs_only: 입력만 변환할지 여부\n",
    "        \n",
    "        Returns:\n",
    "            변환된 새로운 ArcDataset\n",
    "        \"\"\"\n",
    "        queries = {}\n",
    "        replies = {}\n",
    "        keys    = []\n",
    "        for k0 in self.keys:\n",
    "            # 변환 설명자 생성\n",
    "            desc = (('copy{i}' if mod_func is np.copy else mod_func.__name__) if descriptor is None else descriptor if isinstance(descriptor, str) else descriptor(self.queries[k0])).format(i=i)\n",
    "            func = lambda a, d: np.asarray(mod_func(a) if descriptor is None else mod_func(a, d)).tolist()\n",
    "            k1 = k0 if keep_key else f\"{k0}.{'I' if inputs_only else ''}{desc}\"\n",
    "            keys.append(k1)\n",
    "            # 쿼리 변환\n",
    "            queries[k1] = {m: [{t: (func(a, desc) if t=='input' or not inputs_only else a) for t, a in x.items()} for x in e] for m, e in self.queries[k0].items()}\n",
    "            # 답안 변환 (입력만 변환하는 경우가 아닐 때)\n",
    "            if k0 in self.replies:\n",
    "                replies[k1] = [func(a, desc) for a in self.replies[k0]]\n",
    "        ret = self.__class__(queries=queries, replies=replies, keys=keys)\n",
    "        return ret\n",
    "\n",
    "    def mod(self, mod_func, descriptor=None, n=1, stack=None, keep=False, keep_key=False, shuffle=False, join=True, inputs_only=False):\n",
    "        \"\"\"\n",
    "        데이터셋에 변환을 적용하는 메인 함수\n",
    "        \n",
    "        Args:\n",
    "            mod_func: 변환 함수\n",
    "            descriptor: 변환 설명자\n",
    "            n: 변환 횟수\n",
    "            stack: 스택 여부 (None이면 자동 결정)\n",
    "            keep: 원본 유지 여부\n",
    "            keep_key: 키 유지 여부\n",
    "            shuffle: 셞플 여부\n",
    "            join: 결합 여부\n",
    "            inputs_only: 입력만 변환할지 여부\n",
    "        \n",
    "        Returns:\n",
    "            변환된 ArcDataset (또는 데이터셋 리스트)\n",
    "        \"\"\"\n",
    "        assert not (keep and keep_key)\n",
    "        cur = self\n",
    "        ret = [cur.shuffled() if shuffle else cur] if keep else []\n",
    "        if stack is None: stack = mod_func.__name__.startswith('rot')  # 회전의 경우 기본적으로 스택\n",
    "        for i in range(n):\n",
    "            cur = (cur if stack else self).mod_single(mod_func, descriptor, i=i, keep_key=keep_key, inputs_only=inputs_only)\n",
    "            ret.append(cur.shuffled() if shuffle else cur)\n",
    "        return self.__class__.append(*ret) if join else ret\n",
    "\n",
    "    def get(self, key, formatter):\n",
    "        \"\"\"\n",
    "        특정 키의 데이터를 포맷된 형태로 가져오는 함수\n",
    "        \n",
    "        Args:\n",
    "            key: 데이터 키\n",
    "            formatter: 포맷터 객체\n",
    "        \n",
    "        Returns:\n",
    "            포맷된 데이터 딕셔너리\n",
    "        \"\"\"\n",
    "        assert formatter.out2_token is None or key in self.replies\n",
    "        train = formatter.fmt_train(self.queries[key]['train'])\n",
    "        query = formatter.fmt_query(self.queries[key]['test'], i=len(self.queries[key]['train']))\n",
    "        reply = formatter.fmt_reply(self.replies[key], self.faulty.get(key)) if key in self.replies else ''\n",
    "        text = train+query+reply if reply else formatter.fmt_train(self.queries[key]['train'], last_is_challenge=True)\n",
    "        return dict(key=key, train=train, query=query, reply=reply, input=train+query, text=text)\n",
    "\n",
    "    def as_list(self, formatter):\n",
    "        \"\"\"\n",
    "        전체 데이터셋을 리스트 형태로 반환하는 함수\n",
    "        \n",
    "        Args:\n",
    "            formatter: 포맷터 객체\n",
    "        \n",
    "        Returns:\n",
    "            포맷된 데이터 리스트\n",
    "        \"\"\"\n",
    "        return [self.get(key, formatter) for key in self.keys]\n",
    "\n",
    "    def as_dataset(self):\n",
    "        \"\"\"HuggingFace Dataset 형태로 변환하는 함수\"\"\"\n",
    "        from datasets import Dataset\n",
    "        return Dataset.from_list([{'key': k, 'query': self.queries[k], 'reply': self.replies[k]} for k in self.keys])\n",
    "\n",
    "    def get_length(self, key, formatter, name, max_of_transposed=False):\n",
    "        \"\"\"\n",
    "        특정 키의 데이터 길이를 계산하는 함수\n",
    "        \n",
    "        Args:\n",
    "            key: 데이터 키\n",
    "            formatter: 포맷터 객체\n",
    "            name: 계산할 부분 ('input' 또는 'reply')\n",
    "            max_of_transposed: 전치된 버전과의 최대값 사용 여부\n",
    "        \n",
    "        Returns:\n",
    "            데이터 길이\n",
    "        \"\"\"\n",
    "        if formatter is None:\n",
    "            # 포맷터가 없는 경우 원시 크기 계산\n",
    "            if   name=='input': return sum(np.prod(np.shape(v)) for v3 in self.queries[key].values() for v2 in v3 for v in v2.values())\n",
    "            elif name=='reply': return sum(np.prod(np.shape(v)) for v in self.replies[key])\n",
    "            else: assert False\n",
    "        else:\n",
    "            # 포맷터를 사용한 토큰 길이 계산\n",
    "            datasets = [self]\n",
    "            if max_of_transposed:\n",
    "                if self.transposed_dataset is None: self.transposed_dataset = self.mod(np.transpose, keep=False, keep_key=True)\n",
    "                datasets.append(self.transposed_dataset)\n",
    "            return max(len(formatter.tokenizer(ds.get(key, formatter=formatter)[name])['input_ids']) for ds in datasets)\n",
    "\n",
    "    def get_lengths(self, formatter, name, max_of_transposed=False):\n",
    "        \"\"\"모든 키의 데이터 길이를 계산하는 함수\"\"\"\n",
    "        return {key: self.get_length(key, formatter=formatter, name=name, max_of_transposed=max_of_transposed) for key in self.keys}\n",
    "\n",
    "    def sorted_by_len(self, reverse=False, **kwargs):\n",
    "        \"\"\"길이순으로 정렬된 데이터셋을 반환하는 함수\"\"\"\n",
    "        new_keys = [key for _, key in sorted([(v, k) for k, v in self.get_lengths(**kwargs).items()], reverse=reverse)]\n",
    "        return self.change_keys(new_keys)\n",
    "\n",
    "    def filter_by_len(self, min_len=0, max_len=float('inf'), **kwargs):\n",
    "        \"\"\"길이로 필터링된 데이터셋을 반환하는 함수\"\"\"\n",
    "        new_keys = [k for k, v in self.get_lengths(**kwargs).items() if min_len<=v<=max_len]\n",
    "        return self.change_keys(new_keys)\n",
    "\n",
    "    def cut_to_query_count(self, max_count, from_end=False):\n",
    "        \"\"\"\n",
    "        쿼리 개수를 제한하는 함수\n",
    "        \n",
    "        Args:\n",
    "            max_count: 최대 쿼리 개수\n",
    "            from_end: 끝에서부터 자를지 여부\n",
    "        \n",
    "        Returns:\n",
    "            쿼리 개수가 제한된 새로운 ArcDataset\n",
    "        \"\"\"\n",
    "        new_queries = {}\n",
    "        for k in self.keys:\n",
    "            new_queries[k] = q = self.queries[k]\n",
    "            while len(q['train'])>max_count: \n",
    "                q['train'] = q['train'][:-1] if from_end else q['train'][1:]\n",
    "        return self.__class__(queries=new_queries, replies=self.replies, keys=[k for k in self.keys])\n",
    "\n",
    "    def cut_to_len(self, formatter, name, max_len, max_new_tokens='auto', from_end=False, quiet=False, **kwargs):\n",
    "        \"\"\"\n",
    "        최대 길이에 맞춰 데이터를 자르는 함수\n",
    "        \n",
    "        Args:\n",
    "            formatter: 포맷터 객체\n",
    "            name: 계산할 부분\n",
    "            max_len: 최대 길이\n",
    "            max_new_tokens: 최대 새 토큰 수\n",
    "            from_end: 끝에서부터 자를지 여부\n",
    "            quiet: 조용한 모드 여부\n",
    "        \n",
    "        Returns:\n",
    "            길이가 조정된 새로운 ArcDataset\n",
    "        \"\"\"\n",
    "        if max_new_tokens:\n",
    "            if max_new_tokens=='auto': max_new_tokens = formatter.max_new_tokens()\n",
    "            max_len_old, max_len = max_len, max_len - max_new_tokens\n",
    "            if not quiet: print(f'*** 작업 크기를 최대 {max_len_old} 토큰 ({max_len} 입력 + {max_new_tokens} 생성)으로 축소 중...')\n",
    "        elif not quiet: print(f'*** 작업 크기를 최대 {max_len} 토큰으로 축소 중...')\n",
    "        \n",
    "        temp_ds = self.change_keys(self.keys)\n",
    "        new_keys = []\n",
    "        new_queries = {}\n",
    "        new_replies = {}\n",
    "        \n",
    "        for key in (self.keys if quiet else tqdm(self.keys, file=sys.stdout)):\n",
    "            reply = temp_ds.replies.get(key)\n",
    "            # 길이가 초과하는 동안 예제를 제거\n",
    "            while max_len<temp_ds.get_length(key, formatter=formatter, name=name, **kwargs):\n",
    "                query = temp_ds.queries[key]\n",
    "                if not key.split('.')[-1].startswith('ex'): \n",
    "                    key = f\"{key}.ex{''.join(map(str, range(len(query['train']))))}\"\n",
    "                key_split = key.split('.')\n",
    "                assert key_split[-1].startswith('ex')\n",
    "                key = '.'.join(key_split[:-1] + [f'ex{key_split[-1][2:-1] if from_end else key_split[-1][3:]}'])\n",
    "                temp_ds.queries[key] = {k: ((v[:-1] if from_end else v[1:]) if k=='train' else v) for k, v in query.items()}\n",
    "                if reply is not None: temp_ds.replies[key] = reply\n",
    "            new_keys.append(key)\n",
    "            new_queries[key] = temp_ds.queries[key]\n",
    "            if reply is not None: new_replies[key] = reply\n",
    "        return self.__class__(keys=new_keys, queries=new_queries, replies=new_replies)\n",
    "\n",
    "    def shuffle_ex(self, perm=None, keep_max=None):\n",
    "        \"\"\"\n",
    "        예제 순서를 섞는 함수\n",
    "        \n",
    "        Args:\n",
    "            perm: 사용할 순열 (None이면 무작위)\n",
    "            keep_max: 유지할 최대 예제 수\n",
    "        \n",
    "        Returns:\n",
    "            예제가 섞인 새로운 ArcDataset\n",
    "        \"\"\"\n",
    "        new_keys = []\n",
    "        new_queries = {}\n",
    "        new_replies = {}\n",
    "        for key in self.keys:\n",
    "            n = len(self.queries[key]['train'])\n",
    "            p = np.random.permutation(n) if perm is None else perm\n",
    "            if keep_max is not None: p = p[:keep_max]\n",
    "            # 키에 예제 순서 정보 추가\n",
    "            new_key = f'{key}.ex' + ('-' if (p.max()>9) else '').join(map(str, p.tolist()))\n",
    "            new_keys.append(new_key)\n",
    "            new_queries[new_key] = {k: (np.array(v, dtype=object)[p].tolist() if k=='train' else v) for k, v in self.queries[key].items()}\n",
    "            if key in self.replies: new_replies[new_key] = self.replies[key]\n",
    "        return self.__class__(queries=new_queries, replies=new_replies, keys=new_keys)\n",
    "\n",
    "    def shuffle_rp(self, keep_max=None):\n",
    "        \"\"\"\n",
    "        테스트 케이스 순서를 섞는 함수\n",
    "        \n",
    "        Args:\n",
    "            keep_max: 유지할 최대 테스트 케이스 수\n",
    "        \n",
    "        Returns:\n",
    "            테스트 케이스가 섞인 새로운 ArcDataset\n",
    "        \"\"\"\n",
    "        new_keys = []\n",
    "        new_queries = {}\n",
    "        new_replies = {}\n",
    "        for key in self.keys:\n",
    "            n = len(self.queries[key]['test'])\n",
    "            p = np.random.permutation(n)\n",
    "            if keep_max is not None: p = p[:keep_max]\n",
    "            # 키에 테스트 케이스 순서 정보 추가\n",
    "            new_key = f'{key}.rp' + ('-' if (p.max()>9) else '').join(map(str, p.tolist()))\n",
    "            new_keys.append(new_key)\n",
    "            new_queries[new_key] = {k: (np.array(v, dtype=object)[p].tolist() if k=='test' else v) for k, v in self.queries[key].items()}\n",
    "            if key in self.replies: new_replies[new_key] = np.array(self.replies[key], dtype=object)[p].tolist()\n",
    "        return self.__class__(queries=new_queries, replies=new_replies, keys=new_keys)\n",
    "\n",
    "    def append_to_keys(self, text):\n",
    "        \"\"\"\n",
    "        모든 키에 텍스트를 추가하는 함수\n",
    "        \n",
    "        Args:\n",
    "            text: 추가할 텍스트\n",
    "        \n",
    "        Returns:\n",
    "            키가 수정된 새로운 ArcDataset\n",
    "        \"\"\"\n",
    "        return self.change_keys([f'{k}{text}' for k in self.keys])\n",
    "\n",
    "    def random_select(self, n):\n",
    "        \"\"\"\n",
    "        n개 그룹 중에서 무작위로 하나씩 선택하는 함수\n",
    "        \n",
    "        Args:\n",
    "            n: 그룹 수\n",
    "        \n",
    "        Returns:\n",
    "            무작위 선택된 새로운 ArcDataset\n",
    "        \"\"\"\n",
    "        keys = np.array(self.keys).reshape(n, -1).T\n",
    "        choice = np.random.randint(0, n, size=[len(keys)])\n",
    "        return self.change_keys(keys[np.arange(len(keys)), choice])\n",
    "\n",
    "    def augment(self, tp=False, rot=False, n=1, perm=None, perm_append=False, shfl_keys=False, shfl_ex=False, seed=None, quiet=False, inputs_only=False):\n",
    "        \"\"\"\n",
    "        데이터 증강을 수행하는 메인 함수\n",
    "        \n",
    "        Args:\n",
    "            tp: 전치 변환 사용 여부 ('rand'면 무작위 선택)\n",
    "            rot: 회전 변환 사용 여부 ('rand'면 무작위 선택)\n",
    "            n: 변환 횟수\n",
    "            perm: 순열 타입 (None, 'rnd_col', 'rnd_all', 'cnt_col', 'cnt_all')\n",
    "            perm_append: 순열 변환을 추가로 유지할지 여부\n",
    "            shfl_keys: 키 셔플 여부\n",
    "            shfl_ex: 예제 셔플 여부\n",
    "            seed: 랜덤 시드\n",
    "            quiet: 조용한 모드 여부\n",
    "            inputs_only: 입력만 변환할지 여부\n",
    "        \n",
    "        Returns:\n",
    "            증강된 새로운 ArcDataset\n",
    "        \"\"\"\n",
    "        if not quiet: print(f\"*** 데이터셋 증강{' (입력만)' if inputs_only else ''} 중...\")\n",
    "        np.random.seed(seed)\n",
    "        d = self\n",
    "        \n",
    "        # 전치 변환\n",
    "        if tp: d = d.mod(np.transpose, keep=True, inputs_only=inputs_only)\n",
    "        if tp=='rand': d = d.random_select(n=2)\n",
    "        \n",
    "        # 회전 변환\n",
    "        if rot: d = d.mod(np.rot90, n=3, keep=True, inputs_only=inputs_only)\n",
    "        if rot=='rand': d = d.random_select(n=4)\n",
    "        \n",
    "        # 순열 변환\n",
    "        if perm is None and n<=1: d = d.shuffled() if shfl_keys else d\n",
    "        else: d = d.mod(*([np.copy] if perm is None else globals()[f\"permute_{perm}\"]), n=n, shuffle=shfl_keys, keep=perm_append, inputs_only=inputs_only)\n",
    "        \n",
    "        # 예제 셔플\n",
    "        np.random.seed(seed)\n",
    "        if shfl_ex: d = d.shuffle_ex()\n",
    "        return d\n",
    "\n",
    "    def remove_replies(self):\n",
    "        \"\"\"답안을 제거한 새로운 데이터셋을 반환하는 함수\"\"\"\n",
    "        return self.__class__(queries=self.queries, replies={}, keys=[k for k in self.keys])\n",
    "\n",
    "    def split_at_pos(self, pos, random_seed=None):\n",
    "        \"\"\"\n",
    "        지정된 위치에서 데이터셋을 분할하는 함수\n",
    "        \n",
    "        Args:\n",
    "            pos: 분할 위치 (정수 또는 비율)\n",
    "            random_seed: 랜덤 시드 (섞기용)\n",
    "        \n",
    "        Returns:\n",
    "            분할된 두 개의 ArcDataset 튜플\n",
    "        \"\"\"\n",
    "        keys = self.keys\n",
    "        if random_seed is not None:\n",
    "            np.random.seed(random_seed)\n",
    "            keys = np.random.permutation(keys)\n",
    "        if isinstance(pos, float): pos = int(pos * len(self.keys) + 0.5)\n",
    "        keys_split = [keys[:pos], keys[pos:]]\n",
    "        return tuple(self.change_keys(new_keys, keep_flags=True) for new_keys in keys_split)\n",
    "\n",
    "    def get_submission(self, results=None):\n",
    "        \"\"\"\n",
    "        제출용 형식의 결과를 생성하는 함수\n",
    "        \n",
    "        Args:\n",
    "            results: 결과 딕셔너리 (선택사항)\n",
    "        \n",
    "        Returns:\n",
    "            제출용 형식의 딕셔너리\n",
    "        \"\"\"\n",
    "        assert self.is_orig==True, '원본 데이터셋에서만 실행해야 합니다.'\n",
    "        # 각 문제마다 2번의 시도 기회를 가진 제출 형식 생성\n",
    "        submission = {k: [{f'attempt_{i+1}': [[0]] for i in range(2)} for _ in range(len(self.queries[k]['test']))] for k in self.keys}\n",
    "        if results is not None: self.fill_submission(results, submission)\n",
    "        return submission\n",
    "\n",
    "    @staticmethod\n",
    "    def fill_submission(results, submission):\n",
    "        \"\"\"\n",
    "        결과를 제출 형식에 채우는 정적 메서드\n",
    "        \n",
    "        Args:\n",
    "            results: 결과 딕셔너리\n",
    "            submission: 제출 형식 딕셔너리\n",
    "        \"\"\"\n",
    "        print(f'*** {len(results)}개 출력에 대한 제출 생성 중...')\n",
    "        for k, v in results.items():\n",
    "            base_id, base_nr = k.split('_')\n",
    "            target_dict = submission[base_id][int(base_nr)]\n",
    "            for i, g in enumerate(v[:len(target_dict)]):\n",
    "                target_dict[f'attempt_{i+1}'] = g.tolist()\n",
    "\n",
    "    def validate_submission(self, submission):\n",
    "        \"\"\"\n",
    "        제출 결과를 검증하는 함수\n",
    "        \n",
    "        Args:\n",
    "            submission: 제출 딕셔너리\n",
    "        \n",
    "        Returns:\n",
    "            점수 (0~1 사이의 값)\n",
    "        \"\"\"\n",
    "        assert self.is_orig==True, '원본 데이터셋에서만 실행해야 합니다.'\n",
    "        score = 0\n",
    "        for k, v in self.replies.items():\n",
    "            for i, r in enumerate(v):\n",
    "                # 두 번의 시도 중 하나라도 맞으면 점수 획득\n",
    "                for attempt in ['attempt_1', 'attempt_2']:\n",
    "                    if np.array_equal(r, submission[k][i][attempt]):\n",
    "                        score += 1 / len(v)\n",
    "                        break\n",
    "        return score\n",
    "\n",
    "def get_class_MyDataCollator(cache=[]):\n",
    "    \"\"\"\n",
    "    커스텀 데이터 콜레이터 클래스를 반환하는 함수 (싱글톤 패턴)\n",
    "    \n",
    "    Args:\n",
    "        cache: 캐시 리스트 (싱글톤 구현용)\n",
    "    \n",
    "    Returns:\n",
    "        MyDataCollator 클래스\n",
    "    \"\"\"\n",
    "    if not cache:\n",
    "        from trl import DataCollatorForCompletionOnlyLM\n",
    "        \n",
    "        class MyDataCollator(DataCollatorForCompletionOnlyLM):\n",
    "            \"\"\"ARC 작업에 특화된 커스텀 데이터 콜레이터\"\"\"\n",
    "            \n",
    "            def setup(self, out2_token_id=None, fault_token_id=None, fault_freq=0, sample_tries=8, mask_first_output=False):\n",
    "                \"\"\"\n",
    "                데이터 콜레이터 설정\n",
    "                \n",
    "                Args:\n",
    "                    out2_token_id: 두 번째 출력 토큰 ID\n",
    "                    fault_token_id: 오류 토큰 ID\n",
    "                    fault_freq: 오류 주입 빈도\n",
    "                    sample_tries: 샘플링 시도 횟수\n",
    "                    mask_first_output: 첫 번째 출력 마스킹 여부\n",
    "                \n",
    "                Returns:\n",
    "                    설정된 self 객체\n",
    "                \"\"\"\n",
    "                self.out2_token_id = out2_token_id\n",
    "                self.fault_token_id = fault_token_id\n",
    "                self.fault_freq = fault_freq\n",
    "                self.sample_tries = sample_tries\n",
    "                self.mask_first_output = mask_first_output\n",
    "                return self\n",
    "\n",
    "            def torch_call(self, examples):\n",
    "                \"\"\"\n",
    "                배치 처리 메인 함수\n",
    "                \n",
    "                Args:\n",
    "                    examples: 예제 리스트\n",
    "                \n",
    "                Returns:\n",
    "                    처리된 배치\n",
    "                \"\"\"\n",
    "                batch = super().torch_call(examples)\n",
    "                \n",
    "                # 두 번째 출력 토큰 처리\n",
    "                if self.out2_token_id is not None:\n",
    "                    assert not self.fault_freq\n",
    "                    for i in range(len(batch['input_ids'])):\n",
    "                        end_pos = ((batch['labels'][i] != -100              ).nonzero().max()).item() + 1\n",
    "                        mid_pos = ((batch['labels'][i] == self.out2_token_id).nonzero().max()).item() + 1\n",
    "                        beg_pos = mid_pos - (end_pos - mid_pos)\n",
    "                        # 첫 번째 출력을 두 번째 출력으로 복사\n",
    "                        batch['labels'][i][beg_pos:mid_pos] = batch['labels'][i][mid_pos:end_pos]\n",
    "                \n",
    "                # 오류 주입 처리\n",
    "                elif self.fault_freq:\n",
    "                    for i in range(len(batch['input_ids'])):\n",
    "                        end_pos = ((batch['labels'][i] != -100).nonzero().max()).item() + 1\n",
    "                        \n",
    "                        # 동적 오류 빈도 계산\n",
    "                        if not isinstance(self.fault_freq, float):\n",
    "                            eos_token_id = batch['labels'][i][end_pos - 1]\n",
    "                            num_examples = (batch['labels'][i] == eos_token_id).sum().item() - 1\n",
    "                            fault_freq = self.fault_freq[num_examples]\n",
    "                        else: \n",
    "                            fault_freq = self.fault_freq\n",
    "                        \n",
    "                        # 확률적 오류 주입\n",
    "                        if random.random() < fault_freq:\n",
    "                            beg_pos = ((batch['labels'][i][:end_pos]==-100).nonzero().max()).item() + 1\n",
    "                            fault_pos = random.randint(beg_pos, end_pos-2)\n",
    "                            fault_tok = batch['labels'][i][fault_pos].item()\n",
    "                            \n",
    "                            # 다른 토큰으로 교체 시도\n",
    "                            for t in range(self.sample_tries):\n",
    "                                new_tok = batch['labels'][i][random.randint(beg_pos, end_pos-2)].item()\n",
    "                                if fault_tok!=new_tok:\n",
    "                                    batch['input_ids'][i][fault_pos] = new_tok\n",
    "                                    # 오류 후 모든 토큰을 오류 토큰으로 마스킹\n",
    "                                    batch['labels'][i][fault_pos+1:end_pos] = self.fault_token_id\n",
    "                                    break\n",
    "                \n",
    "                # 첫 번째 출력 마스킹\n",
    "                for i in range(len(batch['labels'])):\n",
    "                    for _ in range(self.mask_first_output):\n",
    "                        beg_pos = ((batch['labels'][i] != -100).nonzero().min()).item()\n",
    "                        mid_pos = ((batch['labels'][i][beg_pos:] == -100).nonzero().min()).item() + beg_pos\n",
    "                        end_pos = ((batch['labels'][i] != -100).nonzero().max()).item() + 1\n",
    "                        if mid_pos<end_pos: batch['labels'][i][beg_pos:mid_pos] = -100\n",
    "                return batch\n",
    "        cache.append(MyDataCollator)\n",
    "    return cache[0]\n",
    "\n",
    "class ArcFormatter(object):\n",
    "    \"\"\"ARC 데이터를 텍스트 형식으로 포맷팅하는 클래스\"\"\"\n",
    "    \n",
    "    def __init__(self, inp_prefix, out_prefix, arr_sep, out2_use=False, out2_token=None, arr_beg='', arr_end='', pretext='', pre_out=None, exa_sep='', exa_end='', qry_prefix=None, rpl_prefix=None, rpl_sep=None, dec_sep=None, min_wid=0, min_pad='', pretext_corpus_split='', masking=0, tokenizer=None, collator_kwargs={}, repeat_input_aug=None, repeat_input_pre=None):\n",
    "        \"\"\"\n",
    "        ArcFormatter 초기화\n",
    "        \n",
    "        Args:\n",
    "            inp_prefix: 입력 접두사\n",
    "            out_prefix: 출력 접두사\n",
    "            arr_sep: 배열 분리자\n",
    "            out2_use: 두 번째 출력 사용 여부\n",
    "            out2_token: 두 번째 출력 토큰\n",
    "            arr_beg: 배열 시작 문자\n",
    "            arr_end: 배열 끝 문자\n",
    "            pretext: 전문(前文)\n",
    "            pre_out: 출력 전 텍스트\n",
    "            exa_sep: 예제 분리자\n",
    "            exa_end: 예제 끝 문자\n",
    "            qry_prefix: 쿼리 접두사\n",
    "            rpl_prefix: 응답 접두사\n",
    "            rpl_sep: 응답 분리자\n",
    "            dec_sep: 디코딩 분리자\n",
    "            min_wid: 최소 너비\n",
    "            min_pad: 최소 패딩 문자\n",
    "            pretext_corpus_split: 전문 코퍼스 분할 문자\n",
    "            masking: 마스킹 모드\n",
    "            tokenizer: 토크나이저\n",
    "            collator_kwargs: 콜레이터 인자들\n",
    "            repeat_input_aug: 입력 반복 증강 함수\n",
    "            repeat_input_pre: 입력 반복 접두사\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inp_prefix = inp_prefix\n",
    "        self.out_prefix = out_prefix\n",
    "        self.out2_token = out2_token\n",
    "        self.out2_use = out2_use\n",
    "        assert not out2_use or out2_token is not None\n",
    "        assert not out2_use or masking in [1, 2]\n",
    "        assert masking!=2 or out2_use or rpl_prefix is not None\n",
    "        \n",
    "        # 기본값 설정\n",
    "        self.qry_prefix = qry_prefix if qry_prefix is not None else inp_prefix\n",
    "        self.rpl_prefix = rpl_prefix if rpl_prefix is not None else out_prefix\n",
    "        self.rpl_sep = rpl_sep if rpl_sep is not None else self.rpl_prefix\n",
    "        self.arr_sep = arr_sep\n",
    "        self.arr_beg = arr_beg\n",
    "        self.arr_end = arr_end\n",
    "        self.pretext = pretext\n",
    "        self.pre_out = pre_out\n",
    "        self.pre_out_empty = ['']*99  # 빈 출력 전 텍스트\n",
    "        self.pretext_corpus_split = pretext_corpus_split\n",
    "        self.exa_sep = exa_sep\n",
    "        self.exa_end = exa_end\n",
    "        self.dec_sep = arr_sep if dec_sep is None else dec_sep\n",
    "        self.min_wid = min_wid\n",
    "        self.min_pad = min_pad\n",
    "        self.masking = masking\n",
    "        self.collator_kwargs = collator_kwargs\n",
    "        self.repeat_input_aug = repeat_input_aug\n",
    "        self.repeat_input_pre = repeat_input_pre\n",
    "\n",
    "    def fmt_array(self, array):\n",
    "        \"\"\"\n",
    "        2D 배열을 텍스트 형식으로 포맷팅하는 함수\n",
    "        \n",
    "        Args:\n",
    "            array: 2D 배열\n",
    "        \n",
    "        Returns:\n",
    "            포맷된 텍스트 문자열\n",
    "        \"\"\"\n",
    "        return self.arr_beg + self.arr_sep.join(\n",
    "            str(row).replace(' ', '').replace(',', '').replace('[', '').replace(']', '') + \n",
    "            self.min_pad*max(0, self.min_wid-len(row)) \n",
    "            for row in array\n",
    "        ) + self.arr_end\n",
    "\n",
    "    def get_pre_out(self, pretext_split):\n",
    "        \"\"\"\n",
    "        출력 전 텍스트를 가져오는 함수\n",
    "        \n",
    "        Args:\n",
    "            pretext_split: 전문 분할 여부\n",
    "        \n",
    "        Returns:\n",
    "            출력 전 텍스트 리스트\n",
    "        \"\"\"\n",
    "        if self.pre_out is None: return self.pre_out_empty\n",
    "        if pretext_split: return [self.pretext_corpus_split.join(list(p) + ['']) for p in self.pre_out]\n",
    "        return self.pre_out\n",
    "\n",
    "    def fmt_train(self, train, last_is_challenge=False, pretext_split=False):\n",
    "        \"\"\"\n",
    "        훈련 예제들을 포맷팅하는 함수\n",
    "        \n",
    "        Args:\n",
    "            train: 훈련 예제 리스트\n",
    "            last_is_challenge: 마지막이 도전 문제인지 여부\n",
    "            pretext_split: 전문 분할 여부\n",
    "        \n",
    "        Returns:\n",
    "            포맷된 훈련 데이터 문자열\n",
    "        \"\"\"\n",
    "        po = self.get_pre_out(pretext_split=pretext_split)\n",
    "        ex = []\n",
    "        for i, x in enumerate(train):\n",
    "            if last_is_challenge and i+1==len(train):\n",
    "                # 마지막이 도전 문제인 경우\n",
    "                formatted_ex = f\"{self.fmt_query([x], i, pretext_split=pretext_split)}{self.fmt_reply([x['output']])}\"\n",
    "            else:\n",
    "                # 일반 훈련 예제\n",
    "                formatted_ex = f\"{self.inp_prefix}{self.fmt_array(x['input'])}{self.repeat_input(x, no_aug=pretext_split)}{po[i]}{self.out_prefix}{self.fmt_array(x['output'])}\"\n",
    "            ex.append(formatted_ex)\n",
    "        \n",
    "        pre = self.pretext_corpus_split.join(list(self.pretext)+['']) if pretext_split else self.pretext\n",
    "        end = '' if last_is_challenge else (self.exa_end + self.tokenizer.eos_token)\n",
    "        return pre + (self.exa_end + self.tokenizer.eos_token + self.exa_sep).join(ex) + end\n",
    "\n",
    "    def fmt_query(self, query, i, pretext_split=False):\n",
    "        \"\"\"\n",
    "        쿼리를 포맷팅하는 함수\n",
    "        \n",
    "        Args:\n",
    "            query: 쿼리 리스트\n",
    "            i: 인덱스\n",
    "            pretext_split: 전문 분할 여부\n",
    "        \n",
    "        Returns:\n",
    "            포맷된 쿼리 문자열\n",
    "        \"\"\"\n",
    "        po = self.get_pre_out(pretext_split=pretext_split)\n",
    "        return ''.join(f\"{self.qry_prefix}{self.fmt_array(x['input'])}{self.repeat_input(x, no_aug=pretext_split)}{po[i]}{self.rpl_prefix}\" for x in query[:1])\n",
    "\n",
    "    def repeat_input(self, x, no_aug=False):\n",
    "        \"\"\"\n",
    "        입력 반복 기능\n",
    "        \n",
    "        Args:\n",
    "            x: 입력 데이터\n",
    "            no_aug: 증강 없음 여부\n",
    "        \n",
    "        Returns:\n",
    "            반복된 입력 문자열\n",
    "        \"\"\"\n",
    "        if self.repeat_input_aug is None: return ''\n",
    "        return f\"{self.repeat_input_pre}{self.fmt_array(((lambda x: x) if no_aug else self.repeat_input_aug)(x['input']))}\"\n",
    "\n",
    "    def fmt_reply(self, reply, fault=None):\n",
    "        \"\"\"\n",
    "        응답을 포맷팅하는 함수\n",
    "        \n",
    "        Args:\n",
    "            reply: 응답 리스트\n",
    "            fault: 오류 데이터 (선택사항)\n",
    "        \n",
    "        Returns:\n",
    "            포맷된 응답 문자열\n",
    "        \"\"\"\n",
    "        ids = self.fmt_array(reply[0]) + self.exa_end + self.tokenizer.eos_token\n",
    "        if self.out2_use:\n",
    "            # 두 번째 출력 사용 시\n",
    "            if fault is None: fault = reply\n",
    "            ids = self.fmt_array(fault[0]) + self.exa_end + self.out2_token + ids\n",
    "        return ids\n",
    "\n",
    "    def quick_test(self, decoded, done):\n",
    "        \"\"\"\n",
    "        디코딩된 결과에 대한 빠른 테스트\n",
    "        \n",
    "        Args:\n",
    "            decoded: 디코딩된 문자열\n",
    "            done: 완료 여부\n",
    "        \n",
    "        Returns:\n",
    "            테스트 통과 여부\n",
    "        \"\"\"\n",
    "        sp = decoded.split(self.tokenizer.eos_token)[0].split(self.dec_sep)\n",
    "        sl = len(sp[0])\n",
    "        is_prefix = sl>0 and len(sp[-1])<=sl and (len(sp)==1 or len(sp[-2])==sl) and all(x.isdigit() for x in sp[-1])\n",
    "        return is_prefix and (not done or len(sp[-1])==0 or len(sp[-1])==sl)\n",
    "\n",
    "    @staticmethod\n",
    "    def is_valid_solution(guess):\n",
    "        \"\"\"\n",
    "        추측이 유효한 솔루션인지 확인하는 정적 메서드\n",
    "        \n",
    "        Args:\n",
    "            guess: 추측 배열\n",
    "        \n",
    "        Returns:\n",
    "            유효성 여부\n",
    "        \"\"\"\n",
    "        return isinstance(guess, np.ndarray) and guess.ndim == 2 and all(0 < x <= 30 for x in guess.shape)\n",
    "\n",
    "    def max_new_tokens(self, safety_margin=1):\n",
    "        \"\"\"\n",
    "        최대 새 토큰 수를 계산하는 함수\n",
    "        \n",
    "        Args:\n",
    "            safety_margin: 안전 마진\n",
    "        \n",
    "        Returns:\n",
    "            최대 새 토큰 수\n",
    "        \"\"\"\n",
    "        # 최대 크기 응답 (30x30)으로 계산\n",
    "        max_sized_reply = np.zeros([30, 30], dtype=int)\n",
    "        tokenized = self.tokenizer(self.fmt_reply([max_sized_reply]))['input_ids']\n",
    "        max_new_tokens = len(tokenized)\n",
    "        if tokenized[0]==self.tokenizer.bos_token_id: max_new_tokens -= 1\n",
    "        return max_new_tokens + safety_margin\n",
    "\n",
    "    def de_tokenize(self, tokens, scores=None):\n",
    "        \"\"\"\n",
    "        토큰을 디토크나이즈하는 함수\n",
    "        \n",
    "        Args:\n",
    "            tokens: 토큰 배열\n",
    "            scores: 점수 배열 (선택사항)\n",
    "        \n",
    "        Returns:\n",
    "            (출력 길이, 점수 값, 디토크나이즈된 텍스트, 점수들) 튜플\n",
    "        \"\"\"\n",
    "        import torch\n",
    "        tokens_cut = cut_at_token(tokens, self.tokenizer.eos_token_id)\n",
    "        de_tokenized = self.tokenizer.batch_decode([tokens_cut])[0]\n",
    "        score_val = None\n",
    "        \n",
    "        if scores is not None:\n",
    "            tokens_with_eos = tokens[:len(tokens_cut)+1]\n",
    "            # 로그 소프트맥스로 점수 값 계산\n",
    "            score_val = torch.nn.functional.log_softmax(torch.tensor(scores), dim=-1).numpy().copy()[np.arange(len(tokens_with_eos)), tokens_with_eos].sum()\n",
    "            \n",
    "            # 숫자 토큰들만 추출\n",
    "            number_token_ids = [self.tokenizer.vocab[k] for k in map(str, range(10))]\n",
    "            fault_token_id = self.collator_kwargs.get('fault_token_id')\n",
    "            if fault_token_id is not None: number_token_ids.append(fault_token_id)\n",
    "            number_token_ids = np.array(number_token_ids)\n",
    "            number_positions = (tokens_cut[..., np.newaxis] == number_token_ids).any(-1)\n",
    "            scores = scores[:len(tokens_cut), number_token_ids][number_positions]\n",
    "            scores = torch.nn.functional.log_softmax(torch.tensor(scores), dim=-1)[:, :10].numpy().copy()\n",
    "        \n",
    "        return max(len(tokens)+1, len(tokens_cut)), score_val, de_tokenized, scores\n",
    "\n",
    "    def decode_to_array_single(self, text, score=None, limit_rows=30):\n",
    "        \"\"\"\n",
    "        단일 텍스트를 배열로 디코딩하는 함수\n",
    "        \n",
    "        Args:\n",
    "            text: 디코딩할 텍스트\n",
    "            score: 점수 배열 (선택사항)\n",
    "            limit_rows: 최대 행 수 제한\n",
    "        \n",
    "        Returns:\n",
    "            디코딩 결과 딕셔너리\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # 텍스트를 행별로 분할하고 숫자만 추출\n",
    "            by_rows = [row for row in [[int(x) for x in line if x.isdigit()] for line in text.split(self.dec_sep)] if len(row)]\n",
    "            if limit_rows and len(by_rows) > limit_rows:\n",
    "                by_rows = by_rows[:limit_rows]\n",
    "                limited = True\n",
    "            else: \n",
    "                limited = False\n",
    "            \n",
    "            decoded = np.array(by_rows, dtype=int)\n",
    "            if self.is_valid_solution(decoded):\n",
    "                try:\n",
    "                    assert score is not None\n",
    "                    decoded_flat = decoded.ravel()\n",
    "                    if limited: score = score[:len(decoded_flat)]\n",
    "                    \n",
    "                    # 다양한 점수 형태 계산\n",
    "                    score_all = score.reshape(decoded.shape + score.shape[1:])\n",
    "                    score_result = score[range(len(decoded_flat)), decoded_flat]\n",
    "                    score_reshaped = score_result.reshape(decoded.shape)\n",
    "                    score_cum_reshaped = score_result.cumsum().reshape(score_reshaped.shape)\n",
    "                    score_all_cum = score_cum_reshaped[..., np.newaxis] - score_reshaped[..., np.newaxis] + score_all\n",
    "                except: \n",
    "                    # 점수 계산 실패 시 무한대 값으로 채움\n",
    "                    score_reshaped = score_cum_reshaped = np.full(decoded.shape, -float('inf'))\n",
    "                \n",
    "                return {\n",
    "                    'output': decoded, \n",
    "                    'score': score_reshaped, \n",
    "                    'score_cum': score_cum_reshaped, \n",
    "                    'score_all': score_all, \n",
    "                    'score_all_cum': score_all_cum\n",
    "                }\n",
    "        except: \n",
    "            pass\n",
    "        return {}\n",
    "\n",
    "    def decode_to_array(self, text, score=None, limit_rows=30):\n",
    "        \"\"\"\n",
    "        텍스트를 배열로 디코딩하는 메인 함수\n",
    "        \n",
    "        Args:\n",
    "            text: 디코딩할 텍스트\n",
    "            score: 점수 배열 (선택사항)\n",
    "            limit_rows: 최대 행 수 제한\n",
    "        \n",
    "        Returns:\n",
    "            디코딩 결과 리스트\n",
    "        \"\"\"\n",
    "        if not self.out2_use: \n",
    "            text, score = [text], [score]\n",
    "        else:\n",
    "            # 두 번째 출력 토큰으로 분할\n",
    "            text = text.split(self.out2_token)\n",
    "            if score is None: \n",
    "                score = [None]*len(text)\n",
    "            else:\n",
    "                # 텍스트 길이에 따라 점수 분할\n",
    "                lengths = np.cumsum([len(list(filter(str.isdigit, t))) for t in text])\n",
    "                score = [score[s:e] for s, e in zip([0]+lengths[:-1].tolist(), lengths)]\n",
    "        \n",
    "        return [self.decode_to_array_single(t, s) for t, s in zip(text, score)]\n",
    "\n",
    "    def get_corpus(self):\n",
    "        \"\"\"\n",
    "        토크나이저 학습용 코퍼스를 생성하는 함수\n",
    "        \n",
    "        Returns:\n",
    "            코퍼스 텍스트\n",
    "        \"\"\"\n",
    "        try:\n",
    "            old_min_wid, self.min_wid = self.min_wid, min(self.min_wid, 2)\n",
    "            # 0-9 숫자로 구성된 간단한 예제 생성\n",
    "            return self.fmt_train([{'input': [[i] for i in range(10)], 'output': [[i] for i in range(10)]}]*3, last_is_challenge=True, pretext_split=True)\n",
    "        finally: \n",
    "            self.min_wid = old_min_wid\n",
    "\n",
    "    def get_data_collator(self):\n",
    "        \"\"\"\n",
    "        데이터 콜레이터를 생성하는 함수\n",
    "        \n",
    "        Returns:\n",
    "            데이터 콜레이터 객체 또는 None\n",
    "        \"\"\"\n",
    "        if not self.masking: return None\n",
    "        \n",
    "        from transformers import DataCollatorForLanguageModeling\n",
    "        collator_params = dict(tokenizer=self.tokenizer, mlm=False)\n",
    "        \n",
    "        # 두 번째 출력 토큰 ID 설정\n",
    "        pass_out2_token = self.tokenizer.vocab[self.out2_token] if self.out2_use and self.masking==1 else None\n",
    "        \n",
    "        if self.masking:\n",
    "            assert not self.collator_kwargs.get('mask_first_output') or self.masking==1\n",
    "            # 커스텀 콜레이터 생성\n",
    "            data_collator = get_class_MyDataCollator()(\n",
    "                **collator_params,\n",
    "                instruction_template=[self.inp_prefix, self.tokenizer.bos_token][self.masking - 1],\n",
    "                response_template=[self.out_prefix, (self.out2_token if self.out2_use else self.rpl_sep)][self.masking - 1],\n",
    "            ).setup(out2_token_id=pass_out2_token, **self.collator_kwargs)\n",
    "        else:\n",
    "            assert not self.collator_kwargs, '마스킹이 켜져있을 때만 지원됩니다'\n",
    "            data_collator = DataCollatorForLanguageModeling(**collator_params)\n",
    "        \n",
    "        return data_collator\n",
    "\n",
    "    def get_output_token_ids(self):\n",
    "        \"\"\"\n",
    "        출력에 사용되는 토큰 ID들을 반환하는 함수\n",
    "        \n",
    "        Returns:\n",
    "            출력 토큰 ID 리스트\n",
    "        \"\"\"\n",
    "        assert not self.out2_use\n",
    "        # 숫자 토큰들 (0-9)\n",
    "        num_tokens = [self.tokenizer.vocab[str(i)] for i in range(10)]\n",
    "        \n",
    "        # 분리자 토큰들\n",
    "        sep_tokens = []\n",
    "        for txt in [self.arr_beg, self.arr_sep, self.arr_end, self.exa_sep]:\n",
    "            if txt:\n",
    "                for tok in self.tokenizer(txt)['input_ids'][1:]:\n",
    "                    sep_tokens.append(tok)\n",
    "        sep_tokens.append(self.tokenizer.eos_token_id)\n",
    "        \n",
    "        return num_tokens + sorted(set(sep_tokens))\n",
    "\n",
    "# 사전 정의된 포맷터들\n",
    "ArcFormatter_pretext2 = lambda **kwargs: ArcFormatter(\n",
    "    masking=1, \n",
    "    inp_prefix='I', \n",
    "    out_prefix='O', \n",
    "    arr_sep='\\n', \n",
    "    arr_end='\\n', \n",
    "    pretext='ABCDEFGHJKLMNPQRSTUVWXYZ',  # I와 O를 제외한 알파벳\n",
    "    pretext_corpus_split='\\n', \n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "ArcFormatter_pretext3 = lambda **kwargs: ArcFormatter(\n",
    "    masking=1, \n",
    "    inp_prefix='I', \n",
    "    out_prefix='O', \n",
    "    arr_sep='\\n', \n",
    "    arr_end='\\n', \n",
    "    pretext='ABCDEFGHJKLMNPQRSTUVWXYZabcdefghjklmnpqrstuvwxyz',  # 대소문자 알파벳 (I, O, i, o 제외)\n",
    "    pretext_corpus_split='\\n', \n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "ArcFormatter_premix_2 = lambda **kwargs: ArcFormatter(\n",
    "    masking=1, \n",
    "    inp_prefix='I', \n",
    "    out_prefix='O', \n",
    "    arr_sep='\\n', \n",
    "    arr_end='\\n', \n",
    "    pretext='ABCDEFGHJKLMNPQRSTUVWXYZ', \n",
    "    pre_out=['+/-=']*99,  # 출력 전에 수학 기호 추가\n",
    "    pretext_corpus_split='\\n', \n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "ArcFormatter_premix_3 = lambda **kwargs: ArcFormatter(\n",
    "    masking=1, \n",
    "    inp_prefix='I', \n",
    "    out_prefix='O', \n",
    "    arr_sep='\\n', \n",
    "    arr_end='\\n', \n",
    "    pretext='ABCDEFGHJKLMNPQRSTUVWXYZabcdefghjklmnpqrstuvwxyz', \n",
    "    pre_out=['+/-=']*99,  # 출력 전에 수학 기호 추가\n",
    "    pretext_corpus_split='\\n', \n",
    "    **kwargs\n",
    ")\n",
    "\n",
    "# 사용 가능한 포맷터들의 딕셔너리\n",
    "available_formatters = dict(\n",
    "    ArcFormatter_pretext2=ArcFormatter_pretext2,\n",
    "    ArcFormatter_pretext3=ArcFormatter_pretext3,\n",
    "    ArcFormatter_premix_2=ArcFormatter_premix_2,\n",
    "    ArcFormatter_premix_3=ArcFormatter_premix_3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20325946",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T02:27:57.619077Z",
     "iopub.status.busy": "2025-07-07T02:27:57.618718Z",
     "iopub.status.idle": "2025-07-07T02:27:57.626145Z",
     "shell.execute_reply": "2025-07-07T02:27:57.625565Z"
    },
    "papermill": {
     "duration": 0.016353,
     "end_time": "2025-07-07T02:27:57.627605",
     "exception": false,
     "start_time": "2025-07-07T02:27:57.611252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile selection.py\n",
    "import numpy as np\n",
    "\n",
    "def hashable(guess):\n",
    "    \"\"\"\n",
    "    2D 배열을 해시 가능한 튜플로 변환하는 함수\n",
    "    \n",
    "    Args:\n",
    "        guess: 2D numpy 배열 형태의 추측\n",
    "    \n",
    "    Returns:\n",
    "        중첩 튜플 형태의 해시 가능한 객체\n",
    "    \"\"\"\n",
    "    return tuple(map(tuple, guess))\n",
    "\n",
    "def make_unique(guess_list, indices=None):\n",
    "    \"\"\"\n",
    "    추측 리스트에서 중복을 제거하는 함수\n",
    "    \n",
    "    Args:\n",
    "        guess_list: 추측들의 리스트\n",
    "        indices: 인덱스 리스트 (선택사항)\n",
    "    \n",
    "    Returns:\n",
    "        중복이 제거된 추측 리스트 (또는 추측과 인덱스 튜플)\n",
    "    \"\"\"\n",
    "    used = set()  # 이미 사용된 해시값들을 저장\n",
    "    out = []      # 고유한 추측들\n",
    "    out_ind = []  # 고유한 추측들의 인덱스\n",
    "    \n",
    "    for i, g in enumerate(guess_list):\n",
    "        h = hashable(g)\n",
    "        if h not in used:\n",
    "            used.add(h)\n",
    "            out.append(np.array(g))\n",
    "            if indices is not None: \n",
    "                out_ind.append(indices[i])\n",
    "    \n",
    "    return out if indices is None else (out, out_ind)\n",
    "\n",
    "def first_only(guesses):\n",
    "    \"\"\"\n",
    "    첫 번째 추측만 반환하는 선택 알고리즘\n",
    "    \n",
    "    Args:\n",
    "        guesses: 추측 딕셔너리 {key: {'output': array, ...}}\n",
    "    \n",
    "    Returns:\n",
    "        첫 번째 추측만 포함된 리스트\n",
    "    \"\"\"\n",
    "    return [g['output'] for g in guesses.values()][:1]\n",
    "\n",
    "def keep_order(guesses):\n",
    "    \"\"\"\n",
    "    모든 추측을 원래 순서대로 유지하는 선택 알고리즘\n",
    "    \n",
    "    Args:\n",
    "        guesses: 추측 딕셔너리\n",
    "    \n",
    "    Returns:\n",
    "        모든 추측의 출력 배열 리스트\n",
    "    \"\"\"\n",
    "    return [g['output'] for g in guesses.values()]\n",
    "\n",
    "def keep_order_unique(guesses):\n",
    "    \"\"\"\n",
    "    원래 순서를 유지하면서 중복을 제거하는 선택 알고리즘\n",
    "    \n",
    "    Args:\n",
    "        guesses: 추측 딕셔너리\n",
    "    \n",
    "    Returns:\n",
    "        중복이 제거된 고유한 추측들의 리스트\n",
    "    \"\"\"\n",
    "    return make_unique(keep_order(guesses))\n",
    "\n",
    "def get_best_shape_by_score(guess_list, getter, once_per_result=True):\n",
    "    \"\"\"\n",
    "    점수 기반으로 최고의 형태(shape)를 찾는 함수\n",
    "    \n",
    "    Args:\n",
    "        guess_list: 추측 리스트\n",
    "        getter: 점수를 계산하는 함수\n",
    "        once_per_result: 동일한 결과당 한 번만 계산할지 여부\n",
    "    \n",
    "    Returns:\n",
    "        (최고 점수, 형태, 인덱스들) 튜플\n",
    "    \"\"\"\n",
    "    seen_outputs = set()  # 이미 본 출력들\n",
    "    shape_scores = {}     # 형태별 점수와 인덱스 저장\n",
    "    \n",
    "    for i, g in enumerate(guess_list):\n",
    "        shape = tuple(g['output'].shape)  # 배열의 형태 (높이, 너비)\n",
    "        scores = shape_scores[shape] = shape_scores.get(shape, [[], []])\n",
    "        scores[1].append(i)  # 인덱스 추가\n",
    "        \n",
    "        h = hashable(g['output'])\n",
    "        if h in seen_outputs: continue\n",
    "        if once_per_result: seen_outputs.add(h)\n",
    "        scores[0].append(g)  # 추측 추가\n",
    "    \n",
    "    # 각 형태별로 점수 계산 후 정렬\n",
    "    shape_scores = [(getter(scores), shape, indices) for shape, (scores, indices) in shape_scores.items()]\n",
    "    shape_scores = sorted(shape_scores, key=(lambda x: x[0]), reverse=True)\n",
    "    return shape_scores[0]  # 최고 점수의 형태 반환\n",
    "\n",
    "def score_sum(guesses, getter, shape_getter=None, prefer_common_shape=True):\n",
    "    \"\"\"\n",
    "    점수 합계를 기반으로 추측을 정렬하는 일반적인 함수\n",
    "    \n",
    "    Args:\n",
    "        guesses: 추측 딕셔너리\n",
    "        getter: 점수를 계산하는 함수\n",
    "        shape_getter: 형태 점수를 계산하는 함수 (기본값: getter와 동일)\n",
    "        prefer_common_shape: 일반적인 형태를 선호할지 여부\n",
    "    \n",
    "    Returns:\n",
    "        점수순으로 정렬된 출력 배열들의 리스트\n",
    "    \"\"\"\n",
    "    if shape_getter is None: shape_getter = getter\n",
    "    guess_list = list(guesses.values())\n",
    "    \n",
    "    # 일반적인 형태를 선호하는 경우, 해당 인덱스들을 찾음\n",
    "    common_shape_indices = set(get_best_shape_by_score(guess_list, shape_getter)[2]) if prefer_common_shape else []\n",
    "    \n",
    "    scores = {}\n",
    "    for i, g in enumerate(guess_list):\n",
    "        h = hashable(g['output'])\n",
    "        # [일반적인_형태_여부, 추측들, 출력_배열]\n",
    "        x = scores[h] = scores.get(h, [i in common_shape_indices, [], g['output']])\n",
    "        x[1].append(g)\n",
    "    \n",
    "    # 점수 계산 및 정렬: (일반적인_형태_여부, 계산된_점수, 출력_배열)\n",
    "    scores = [(cs, getter(sc), o) for cs, sc, o in scores.values()]\n",
    "    scores = sorted(scores, key=(lambda x: x[:2]), reverse=True)\n",
    "    ordered_outputs = [x[-1] for x in scores]\n",
    "    return ordered_outputs\n",
    "\n",
    "# 확률 합계를 계산하는 getter 함수\n",
    "getter_all_probsum = lambda guesses: sum(np.exp(g['score_val']) for g in guesses)\n",
    "\n",
    "def score_all_probsum(guesses): \n",
    "    \"\"\"\n",
    "    모든 확률의 합계를 기반으로 추측을 선택하는 알고리즘\n",
    "    \n",
    "    Args:\n",
    "        guesses: 추측 딕셔너리\n",
    "    \n",
    "    Returns:\n",
    "        확률 합계순으로 정렬된 추측들\n",
    "    \"\"\"\n",
    "    return score_sum(guesses, getter_all_probsum)\n",
    "\n",
    "def getter_full_probmul(p):\n",
    "    \"\"\"\n",
    "    전체 확률 곱셈을 위한 getter 생성 함수\n",
    "    \n",
    "    Args:\n",
    "        p: 기준선(baseline) 값\n",
    "    \n",
    "    Returns:\n",
    "        확률 곱셈을 계산하는 getter 함수\n",
    "    \"\"\"\n",
    "    def _getter(guesses, baseline=p):\n",
    "        \"\"\"\n",
    "        추론 점수와 증강 점수를 결합하여 전체 점수를 계산\n",
    "        \n",
    "        Args:\n",
    "            guesses: 추측 리스트\n",
    "            baseline: 기준선 값\n",
    "        \n",
    "        Returns:\n",
    "            결합된 점수\n",
    "        \"\"\"\n",
    "        # 추론 점수: 각 추측의 점수에 기준선을 더한 합\n",
    "        inf_score = sum([g['score_val']+baseline for g in guesses])\n",
    "        \n",
    "        # 증강 점수: 다중 점수들의 평균 (기준선 포함)\n",
    "        aug_score = np.mean([sum(s+baseline for s in g['score_multi_nl']) for g in guesses])\n",
    "        \n",
    "        return inf_score + aug_score\n",
    "    return _getter\n",
    "\n",
    "def score_full_probmul_3(guesses): \n",
    "    \"\"\"\n",
    "    기준선 3을 사용한 전체 확률 곱셈 기반 선택 알고리즘\n",
    "    \n",
    "    Args:\n",
    "        guesses: 추측 딕셔너리\n",
    "    \n",
    "    Returns:\n",
    "        전체 확률 곱셈 점수순으로 정렬된 추측들\n",
    "    \"\"\"\n",
    "    return score_sum(guesses, getter_full_probmul(3), prefer_common_shape=False)\n",
    "\n",
    "# 사용 가능한 선택 알고리즘들의 리스트\n",
    "selection_algorithms = [\n",
    "    first_only,            # 첫 번째만 선택\n",
    "    keep_order,            # 순서 유지\n",
    "    keep_order_unique,     # 순서 유지 + 중복 제거\n",
    "    score_all_probsum,     # 확률 합계 기반\n",
    "    score_full_probmul_3,  # 전체 확률 곱셈 기반 (기준선=3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1200ce2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T02:27:57.642061Z",
     "iopub.status.busy": "2025-07-07T02:27:57.641718Z",
     "iopub.status.idle": "2025-07-07T02:27:57.647133Z",
     "shell.execute_reply": "2025-07-07T02:27:57.646535Z"
    },
    "papermill": {
     "duration": 0.014247,
     "end_time": "2025-07-07T02:27:57.648565",
     "exception": false,
     "start_time": "2025-07-07T02:27:57.634318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile async_tools.py\n",
    "import sys\n",
    "import asyncio\n",
    "\n",
    "async def stream_reader(stream, id, to):\n",
    "    \"\"\"\n",
    "    스트림에서 데이터를 비동기적으로 읽고 실시간으로 출력하는 함수\n",
    "    \n",
    "    Args:\n",
    "        stream: 읽을 스트림 (stdout 또는 stderr)\n",
    "        id: 프로세스 식별자 (None이면 ID 표시 안함)\n",
    "        to: 출력할 대상 스트림 (None이면 출력 안함)\n",
    "    \"\"\"\n",
    "    # ID 접두사 설정 (None이면 빈 문자열)\n",
    "    id = '' if id is None else f'{id}. '\n",
    "    data = b''  # 아직 완성되지 않은 라인의 버퍼\n",
    "    \n",
    "    while True:\n",
    "        # 스트림에서 최대 4096바이트씩 읽기\n",
    "        read = await stream.read(n=4096)\n",
    "        if not read: break  # 더 이상 읽을 데이터가 없으면 종료\n",
    "        \n",
    "        if to is not None:\n",
    "            # 개행 문자로 라인 분할 (마지막에 'X' 추가하여 빈 라인 구분)\n",
    "            *complete_lines, data = (data + read + b'X').splitlines()\n",
    "            data = data[:-1]  # 'X' 제거\n",
    "            \n",
    "            # 완성된 라인들을 출력\n",
    "            for line in complete_lines:\n",
    "                line = line.rstrip()  # 끝의 공백 문자 제거\n",
    "                if line:  # 빈 라인이 아닌 경우에만 출력\n",
    "                    print(f\"{id}{line.decode('utf-8')}\", file=to, end='\\n', flush=True)\n",
    "\n",
    "async def wait_for_subprocess(subprocess, print_output=False, id=None):\n",
    "    \"\"\"\n",
    "    단일 서브프로세스의 완료를 기다리면서 출력을 스트리밍하는 함수\n",
    "    \n",
    "    Args:\n",
    "        subprocess: 기다릴 서브프로세스 객체\n",
    "        print_output: 출력을 콘솔에 표시할지 여부\n",
    "        id: 프로세스 식별자 (다중 프로세스 실행 시 구분용)\n",
    "    \n",
    "    Returns:\n",
    "        서브프로세스의 종료 코드\n",
    "    \"\"\"\n",
    "    # stdout과 stderr를 동시에 비동기적으로 처리\n",
    "    await asyncio.gather(\n",
    "        stream_reader(\n",
    "            subprocess.stdout, \n",
    "            id, \n",
    "            (sys.stdout if print_output else None)  # 출력 표시 여부에 따라 대상 설정\n",
    "        ),\n",
    "        stream_reader(\n",
    "            subprocess.stderr, \n",
    "            id, \n",
    "            (sys.stderr if print_output else None)   # 에러 출력도 동일하게 처리\n",
    "        ),\n",
    "    )\n",
    "    \n",
    "    # 서브프로세스의 종료를 기다리고 종료 코드 반환\n",
    "    return await subprocess.wait()\n",
    "\n",
    "async def wait_for_subprocesses(*processes, print_output=False):\n",
    "    \"\"\"\n",
    "    여러 서브프로세스들의 완료를 동시에 기다리는 함수\n",
    "    \n",
    "    Args:\n",
    "        *processes: 기다릴 서브프로세스들 (가변 인수)\n",
    "        print_output: 출력을 콘솔에 표시할지 여부\n",
    "    \n",
    "    Returns:\n",
    "        모든 서브프로세스들의 종료 코드 리스트\n",
    "    \"\"\"\n",
    "    # 각 프로세스에 대해 wait_for_subprocess를 비동기적으로 실행\n",
    "    # 프로세스가 여러 개인 경우에만 ID 부여 (구분용)\n",
    "    return await asyncio.gather(*[\n",
    "        wait_for_subprocess(\n",
    "            p, \n",
    "            print_output=print_output, \n",
    "            id=i if len(processes) > 1 else None  # 단일 프로세스면 ID 없음\n",
    "        ) \n",
    "        for i, p in enumerate(processes)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af9dec8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T02:27:57.663487Z",
     "iopub.status.busy": "2025-07-07T02:27:57.663250Z",
     "iopub.status.idle": "2025-07-07T02:27:57.674064Z",
     "shell.execute_reply": "2025-07-07T02:27:57.673501Z"
    },
    "papermill": {
     "duration": 0.0201,
     "end_time": "2025-07-07T02:27:57.675488",
     "exception": false,
     "start_time": "2025-07-07T02:27:57.655388",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile common_stuff.py\n",
    "# ARC 훈련 및 평가를 위한 공통 설정\n",
    "from arc_loader import *\n",
    "from model_runner import *\n",
    "from selection import *\n",
    "from async_tools import *\n",
    "import time\n",
    "\n",
    "# ===== 파일 경로 설정 =====\n",
    "tmp_dir = '/kaggle/temp'  # 임시 파일 저장 디렉토리\n",
    "arc_challenge_file = '/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json'  # ARC 테스트 문제 파일\n",
    "arc_solutions_file = '/kaggle/input/arc-prize-2025/arc-agi_training_solutions.json'  # ARC 훈련 솔루션 파일\n",
    "model_temp_storage = os.path.join(tmp_dir, 'finetuned_model')  # 파인튜닝된 모델 저장 경로\n",
    "infer_temp_storage = os.path.join(tmp_dir, 'inference_outputs')  # 추론 결과 저장 경로\n",
    "score_temp_storage = os.path.join(tmp_dir, 'inference_scoring')  # 점수 계산 결과 저장 경로\n",
    "\n",
    "# ===== 데이터셋 로드 =====\n",
    "arc_test_set = ArcDataset.from_file(arc_challenge_file)  # ARC 테스트 세트 로드\n",
    "if arc_test_set.is_fake: arc_test_set.load_replies(arc_solutions_file)  # 가짜 테스트 세트인 경우 솔루션 로드\n",
    "#arc_test_set.is_fake = False  # 전체 실행 강제 (주석 처리됨)\n",
    "#arc_train_set = ArcDataset.from_file('/kaggle/input/arc-prize-2025/arc-agi_training_challenges.json')  # 훈련 세트 (사용 안함)\n",
    "\n",
    "# ===== 모델 설정 =====\n",
    "base_model = '/kaggle/input/wb55l_nemomini_fulleval/transformers/default/1'  # 기본 모델 경로\n",
    "MyFormatter = ArcFormatter_premix_3  # 사용할 포맷터 (수학 기호 포함 버전)\n",
    "perm_aug = 'rnd_all'  # 순열 증강 타입 (모든 색상 무작위 순열)\n",
    "max_seq_length_train = 4224  # 훈련 시 최대 시퀀스 길이\n",
    "mask_first = 0  # 첫 번째 출력 마스킹 설정\n",
    "\n",
    "# ===== 훈련 및 추론 설정 =====\n",
    "train_epochs = 4  # 훈련 에포크 수\n",
    "multi_gpu_train = True  # 다중 GPU 훈련 사용 여부\n",
    "multi_gpu_random_split = True  # 다중 GPU 시 무작위 분할 여부\n",
    "max_seq_length_infer = 8192  # 추론 시 최대 시퀀스 길이\n",
    "prime_on_single_task = False  # 단일 작업 프라이밍 여부\n",
    "infer_params = dict(\n",
    "    min_prob=0.17,  # 터보 DFS 최소 확률 임계값\n",
    "    store=infer_temp_storage,  # 추론 결과 저장 경로\n",
    "    use_turbo=True  # 터보 모드 사용 여부\n",
    ")\n",
    "\n",
    "# ===== 점수 계산 설정 =====\n",
    "use_aug_score = True  # 증강 점수 사용 여부\n",
    "aug_score_params = dict(\n",
    "    tp=True,  # 전치 변환 사용\n",
    "    rot=True,  # 회전 변환 사용\n",
    "    perm=perm_aug,  # 순열 증강 타입\n",
    "    shfl_ex=True,  # 예제 셔플 사용\n",
    "    make_unique=True,  # 고유성 확보\n",
    "    max_len=max_seq_length_infer  # 최대 길이\n",
    ")\n",
    "# 제출용 선택 알고리즘 (증강 점수 사용 여부에 따라 결정)\n",
    "submission_select_algo = score_full_probmul_3 if use_aug_score else score_all_probsum\n",
    "\n",
    "def prepare_run(model_path, load_lora=None, train=False, gpu=None, **kwargs):\n",
    "    \"\"\"\n",
    "    모델 실행을 위한 준비 함수\n",
    "    \n",
    "    Args:\n",
    "        model_path: 모델 경로\n",
    "        load_lora: 로드할 LoRA 경로\n",
    "        train: 훈련 모드 여부\n",
    "        gpu: 사용할 GPU 번호\n",
    "        **kwargs: 추가 인자들\n",
    "    \n",
    "    Returns:\n",
    "        (모델, 포맷터) 튜플\n",
    "    \"\"\"\n",
    "    # GPU 설정\n",
    "    if gpu is not None:\n",
    "        os.environ[\"CUDA_DEVICE_ORDER\"   ] = \"PCI_BUS_ID\"\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu)\n",
    "\n",
    "    # 모델, 토크나이저, 포맷터 준비\n",
    "    model, tokenizer, formatter = prepare_model(\n",
    "        model=model_path,\n",
    "        local_files_only=True,  # 로컬 파일만 사용\n",
    "        mode='unsloth_4bit',  # Unsloth 4비트 모드\n",
    "        #shrink_embedding=8000,  # 임베딩 축소 (주석 처리됨)\n",
    "        max_seq_length=max_seq_length_train,\n",
    "        formatter=MyFormatter,\n",
    "        # LoRA 설정 (훈련 또는 LoRA 로드 시에만)\n",
    "        peft=([dict(\n",
    "            r=64,  # LoRA 랭크 (8, 16, 32, 64, 128 중 선택)\n",
    "            target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj', 'embed_tokens', 'lm_head'],\n",
    "            lora_alpha=16,  # LoRA 알파 값\n",
    "            lora_dropout=0,  # LoRA 드롭아웃 (0이 최적화됨)\n",
    "            bias=\"none\",  # 바이어스 설정 (\"none\"이 최적화됨)\n",
    "            use_gradient_checkpointing=True,  # 그래디언트 체크포인팅 사용\n",
    "            random_state=42,  # 랜덤 시드\n",
    "            use_rslora=True,  # 랭크 안정화 LoRA 사용\n",
    "            loftq_config=None,  # LoftQ 설정\n",
    "        )] if train or load_lora else []) + ([load_lora] if load_lora else []),\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    # 훈련 시 첫 번째 출력 마스킹 설정\n",
    "    if train and mask_first: \n",
    "        formatter.collator_kwargs.update(mask_first_output=mask_first)\n",
    "\n",
    "    return model, formatter\n",
    "\n",
    "def prepare_dataset(formatter, train, gpu=None):\n",
    "    \"\"\"\n",
    "    데이터셋을 준비하는 함수\n",
    "    \n",
    "    Args:\n",
    "        formatter: 포맷터 객체\n",
    "        train: 훈련 모드 여부\n",
    "        gpu: 사용할 GPU 번호\n",
    "    \n",
    "    Returns:\n",
    "        준비된 ArcDataset\n",
    "    \"\"\"\n",
    "    ds = arc_test_set\n",
    "    \n",
    "    # 다중 GPU 훈련 시 데이터 분할\n",
    "    if multi_gpu_train and gpu is not None:\n",
    "        if multi_gpu_random_split:\n",
    "            # 4개 GPU용 무작위 분할\n",
    "            ds = ds.shuffled(seed=123)\n",
    "            split_size = len(ds.keys) // 4\n",
    "            start_idx = gpu * split_size\n",
    "            end_idx = start_idx + split_size if gpu < 3 else len(ds.keys)\n",
    "            ds = ds.change_keys(ds.keys[start_idx:end_idx])\n",
    "        else:\n",
    "            # 길이 기반 분할 (4개 GPU용)\n",
    "            ds = ds.sorted_by_len(formatter=formatter, name='input', max_of_transposed=True)\n",
    "            assignment = ([0,1,2,3]*ds.length())[:ds.length()][::-1]\n",
    "            ds = ds.change_keys((np.array(ds.keys)[np.array(assignment)==gpu]).tolist())\n",
    "    \n",
    "    \n",
    "    if train:\n",
    "        # ===== 훈련용 데이터셋 준비 =====\n",
    "        ds = ds.remove_replies()  # 답안 제거 (자가 지도 학습)\n",
    "        # 데이터 증강\n",
    "        ds = ds.augment(\n",
    "            tp=True,  # 전치 변환\n",
    "            rot=True,  # 회전 변환\n",
    "            perm=perm_aug,  # 순열 증강\n",
    "            n=(2 if arc_test_set.is_fake else train_epochs),  # 증강 횟수\n",
    "            shfl_ex=True,  # 예제 셔플\n",
    "            shfl_keys=True  # 키 셔플\n",
    "        )\n",
    "        # 최대 길이로 자르기 (새 토큰 생성 없음)\n",
    "        ds = ds.cut_to_len(formatter=formatter, name='text', max_len=max_seq_length_train, max_new_tokens=0)\n",
    "        # 가짜 테스트 세트인 경우 긴 것부터 정렬\n",
    "        if arc_test_set.is_fake: \n",
    "            ds = ds.sorted_by_len(formatter=formatter, name='text', reverse=True)\n",
    "    else:\n",
    "        # 추론 시에도 4개 GPU로 분할\n",
    "        ds = ds.sorted_by_len(formatter=formatter, name='input', max_of_transposed=True)\n",
    "        ds = ds.split_multi_replies()\n",
    "        \n",
    "        # 4개 GPU로 균등 분할\n",
    "        if gpu is not None:\n",
    "            total_keys = len(ds.keys)\n",
    "            split_size = total_keys // 4\n",
    "            start_idx = gpu * split_size\n",
    "            end_idx = start_idx + split_size if gpu < 3 else total_keys\n",
    "            ds.keys = ds.keys[start_idx:end_idx]\n",
    "        \n",
    "        # 증강 및 인터리브\n",
    "        ds = ds.augment(tp=True, rot=True, n=2, seed=42, perm=perm_aug, shfl_ex=True).interleave(len(ds.keys))\n",
    "        ds = ds.cut_to_len(formatter=formatter, name='input', max_len=max_seq_length_infer)\n",
    "        \n",
    "        if arc_test_set.is_fake: \n",
    "            ds.keys = ds.keys[:32]  # 각 GPU당 32개씩\n",
    "    \n",
    "    return ds\n",
    "\n",
    "def start_training(gpu):\n",
    "    \"\"\"\n",
    "    지정된 GPU에서 훈련을 시작하는 함수\n",
    "    \n",
    "    Args:\n",
    "        gpu: 사용할 GPU 번호\n",
    "    \"\"\"\n",
    "    try:\n",
    "        storage_path = f'{model_temp_storage}_gpu{gpu}'\n",
    "        # GPU 0이거나 다중 GPU 모드이고, 저장 경로가 없는 경우에만 훈련\n",
    "        if (gpu==0 or multi_gpu_train) and not os.path.exists(storage_path):\n",
    "            with RemapCudaOOM():  # CUDA OOM 에러 처리\n",
    "                # 모델과 포맷터 준비\n",
    "                model, formatter = prepare_run(base_model, train=True, gpu=gpu)\n",
    "                # 데이터셋 준비\n",
    "                dataset = prepare_dataset(formatter, train=True, gpu=gpu if multi_gpu_train else None)\n",
    "                \n",
    "                # 훈련 실행\n",
    "                model, trainer_stats = training_run(\n",
    "                    model, formatter, dataset, \n",
    "                    store=storage_path,  # 모델 저장 경로\n",
    "                    max_seq_length=max_seq_length_train,\n",
    "                    grad_acc_fix=False,  # 그래디언트 누적 수정 비활성화\n",
    "                    train_args=dict(\n",
    "                        per_device_train_batch_size=2,  # 디바이스당 배치 크기\n",
    "                        gradient_accumulation_steps=2,  # 그래디언트 누적 단계\n",
    "                        warmup_steps=100,  # 워밍업 단계\n",
    "                        num_train_epochs=1,  # 훈련 에포크 수\n",
    "                        max_steps=20 if arc_test_set.is_fake else -1,  # 최대 스텝 (가짜 세트는 20스텝만)\n",
    "                        learning_rate=1e-4,  # 학습률\n",
    "                        embedding_learning_rate=1e-5,  # 임베딩 학습률\n",
    "                        logging_steps=10,  # 로깅 간격\n",
    "                        optim=\"adamw_8bit\",  # 8비트 AdamW 옵티마이저\n",
    "                        weight_decay=0.01,  # 가중치 감쇠\n",
    "                        lr_scheduler_type='cosine',  # 코사인 학습률 스케줄러\n",
    "                        seed=42,  # 랜덤 시드\n",
    "                        output_dir=os.path.join(tmp_dir, 'checkpoints'),  # 체크포인트 저장 경로\n",
    "                        save_strategy=\"no\",  # 저장 전략 (저장 안함)\n",
    "                        report_to='none',  # 리포팅 비활성화\n",
    "                    ),\n",
    "                )\n",
    "                mem_info()  # 메모리 정보 출력\n",
    "    finally: \n",
    "        # 훈련 완료 표시 파일 생성\n",
    "        os.makedirs(f'{storage_path}_done', exist_ok=True)\n",
    "\n",
    "def start_inference(gpu):\n",
    "    \"\"\"\n",
    "    지정된 GPU에서 추론을 시작하는 함수\n",
    "    \n",
    "    Args:\n",
    "        gpu: 사용할 GPU 번호\n",
    "    \"\"\"\n",
    "    storage_path = f'{model_temp_storage}_gpu{gpu % 4 if multi_gpu_train else 0}'\n",
    "\n",
    "    \n",
    "    # 훈련 완료까지 대기\n",
    "    while not os.path.exists(f'{storage_path}_done'): \n",
    "        time.sleep(15)\n",
    "    \n",
    "    with RemapCudaOOM():  # CUDA OOM 에러 처리\n",
    "        # 훈련된 모델로 준비\n",
    "        model, formatter = prepare_run(storage_path, gpu=gpu)\n",
    "        # 추론용 데이터셋 준비\n",
    "        dataset = prepare_dataset(formatter, train=False, gpu=gpu)\n",
    "        \n",
    "        # 단일 작업 프라이밍을 위한 재훈련기 설정\n",
    "        retrainer = None if not prime_on_single_task else Retrainer(\n",
    "            n=32,  # 훈련 샘플 수\n",
    "            aug_opts=dict(perm=perm_aug, shfl_ex=True),  # 증강 옵션\n",
    "            reload_state_dict=get_and_fix_peft_weights(storage_path),  # PEFT 가중치 재로드\n",
    "            formatter=formatter,\n",
    "            max_seq_length=max_seq_length_infer,\n",
    "            grad_acc_fix=False,\n",
    "            train_args=dict(\n",
    "                per_device_train_batch_size=2,\n",
    "                gradient_accumulation_steps=2,\n",
    "                warmup_steps=4,\n",
    "                num_train_epochs=1,\n",
    "                learning_rate=1e-4,\n",
    "                embedding_learning_rate=0,  # 임베딩 학습률 0 (고정)\n",
    "                logging_steps=8,\n",
    "                optim=\"adamw_8bit\",\n",
    "                weight_decay=0.00,  # 가중치 감쇠 없음\n",
    "                lr_scheduler_type='constant',  # 상수 학습률\n",
    "                seed=42,\n",
    "                output_dir='tmp_output',\n",
    "                save_strategy='no',\n",
    "                report_to='none',\n",
    "            ),\n",
    "        )\n",
    "        \n",
    "        # 디코더 설정\n",
    "        decoder = Decoder(\n",
    "            formatter, \n",
    "            arc_test_set.split_multi_replies(), \n",
    "            n_guesses=2,  # 최대 추측 횟수\n",
    "            prob_baseline=0.05  # 확률 기준선\n",
    "        )\n",
    "        \n",
    "        # 추론 실행\n",
    "        inference_run_v2(model, formatter, dataset, decoder, retrain=retrainer, **infer_params)\n",
    "        \n",
    "        # 증강 점수 계산 (필요한 경우)\n",
    "        if use_aug_score or arc_test_set.is_fake: \n",
    "            decoder.calc_augmented_scores(model=model, store=score_temp_storage, **aug_score_params)\n",
    "        \n",
    "        mem_info()  # 메모리 정보 출력\n",
    "\n",
    "class RemapCudaOOM:\n",
    "    \"\"\"CUDA Out of Memory 에러를 처리하는 컨텍스트 매니저\"\"\"\n",
    "    \n",
    "    def __enter__(self): \n",
    "        \"\"\"컨텍스트 진입 시 아무것도 하지 않음\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def __exit__(self, exc_type, exc_value, traceback):\n",
    "        \"\"\"\n",
    "        컨텍스트 종료 시 CUDA OOM 에러 처리\n",
    "        \n",
    "        CUDA 메모리 부족 에러가 발생하면 제출 파일을 생성하여\n",
    "        채점 에러를 발생시킴 (Kaggle 환경에서의 안전장치)\n",
    "        \"\"\"\n",
    "        oom_errors = [\n",
    "            \"CUDA out of memory\", \n",
    "            \"Make sure you have enough GPU RAM\", \n",
    "            \"does not fit any GPU's remaining memory\"\n",
    "        ]\n",
    "        if exc_value and any(x in str(exc_value) for x in oom_errors):\n",
    "            # 의도적으로 잘못된 제출 파일 생성\n",
    "            with open('submission.json', 'w') as f: \n",
    "                f.write('cause submission scoring error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8a318f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T02:27:57.689988Z",
     "iopub.status.busy": "2025-07-07T02:27:57.689608Z",
     "iopub.status.idle": "2025-07-07T02:29:36.302233Z",
     "shell.execute_reply": "2025-07-07T02:29:36.301427Z"
    },
    "papermill": {
     "duration": 98.621577,
     "end_time": "2025-07-07T02:29:36.303881",
     "exception": false,
     "start_time": "2025-07-07T02:27:57.682304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from common_stuff import *\n",
    "import os\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "if not os.path.exists(os.path.join(tmp_dir, 'unsloth_installed')):  # unsloth offline install - https://stackoverflow.com/a/51646354\n",
    "    !pip uninstall --yes torch accelerate\n",
    "    !pip install --no-index --find-links=/kaggle/input/unsloth-2024-9-post4/wheelhouse unsloth\n",
    "    #!pip uninstall --yes accelerate fastai torch torchaudio transformers\n",
    "    #!pip install --no-index --find-links=/kaggle/input/unsloth-2024-10-7/wheelhouse unsloth  # do not use grad_acc_fix - trains very slow\n",
    "    #!sed -i 's/if ((post_check - pre_check) >= 1).sum() > 1:/if False:/g' /opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py\n",
    "    # fix delay bug in get_statistics()\n",
    "    !sed -i 's/^def get_statistics():/def get_statistics():\\n if False:/g' /opt/conda/lib/python3.10/site-packages/unsloth/models/_utils.py\n",
    "    # fix faulty unsloth multi-gpu detection\n",
    "    !sed -i \"s/raise RuntimeError('Unsloth currently does not support multi GPU setups - but we are working on it!')/pass/g\" /opt/conda/lib/python3.10/site-packages/unsloth/tokenizer_utils.py /opt/conda/lib/python3.10/site-packages/unsloth/models/llama.py /opt/conda/lib/python3.10/site-packages/unsloth/models/vision.py\n",
    "    os.makedirs(os.path.join(tmp_dir, 'unsloth_installed'), exist_ok=True)\n",
    "    print('Unsloth installed & patched.')\n",
    "\n",
    "for gpu in [0, 1, 2, 3]: \n",
    "    signal_path = f'{model_temp_storage}_gpu{gpu}_done'\n",
    "    if os.path.exists(signal_path): os.rmdir(signal_path)\n",
    "\n",
    "if arc_test_set.is_fake:  # cleanup? (for debugging)\n",
    "    #!rm -R /kaggle/temp/finetuned_model*\n",
    "    #!rm -R /kaggle/temp/inference_outputs\n",
    "    #!rm -R /kaggle/temp/inference_scoring\n",
    "    #!ls /kaggle/temp\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deaa1201",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T02:29:36.323424Z",
     "iopub.status.busy": "2025-07-07T02:29:36.323123Z",
     "iopub.status.idle": "2025-07-07T02:29:36.335474Z",
     "shell.execute_reply": "2025-07-07T02:29:36.334807Z"
    },
    "papermill": {
     "duration": 0.023759,
     "end_time": "2025-07-07T02:29:36.336927",
     "exception": false,
     "start_time": "2025-07-07T02:29:36.313168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python --bg --proc train_proc0\n",
    "from common_stuff import *\n",
    "start_training(gpu=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2371b59d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T02:29:36.356152Z",
     "iopub.status.busy": "2025-07-07T02:29:36.355454Z",
     "iopub.status.idle": "2025-07-07T02:29:36.361168Z",
     "shell.execute_reply": "2025-07-07T02:29:36.360579Z"
    },
    "papermill": {
     "duration": 0.016754,
     "end_time": "2025-07-07T02:29:36.362644",
     "exception": false,
     "start_time": "2025-07-07T02:29:36.345890",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python --bg --proc train_proc1\n",
    "from common_stuff import *\n",
    "start_training(gpu=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "508ec6bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T02:29:36.381204Z",
     "iopub.status.busy": "2025-07-07T02:29:36.380946Z",
     "iopub.status.idle": "2025-07-07T02:29:36.386011Z",
     "shell.execute_reply": "2025-07-07T02:29:36.385433Z"
    },
    "papermill": {
     "duration": 0.015963,
     "end_time": "2025-07-07T02:29:36.387489",
     "exception": false,
     "start_time": "2025-07-07T02:29:36.371526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python --bg --proc train_proc2\n",
    "from common_stuff import *\n",
    "start_training(gpu=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6448f53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T02:29:36.406563Z",
     "iopub.status.busy": "2025-07-07T02:29:36.406018Z",
     "iopub.status.idle": "2025-07-07T02:29:36.411077Z",
     "shell.execute_reply": "2025-07-07T02:29:36.410473Z"
    },
    "papermill": {
     "duration": 0.01599,
     "end_time": "2025-07-07T02:29:36.412494",
     "exception": false,
     "start_time": "2025-07-07T02:29:36.396504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python --bg --proc train_proc3\n",
    "from common_stuff import *\n",
    "start_training(gpu=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b787cf5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T02:29:36.442156Z",
     "iopub.status.busy": "2025-07-07T02:29:36.441640Z",
     "iopub.status.idle": "2025-07-07T02:29:36.452871Z",
     "shell.execute_reply": "2025-07-07T02:29:36.451874Z"
    },
    "papermill": {
     "duration": 0.034787,
     "end_time": "2025-07-07T02:29:36.456247",
     "exception": false,
     "start_time": "2025-07-07T02:29:36.421460",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python --bg --proc infer_proc0\n",
    "from common_stuff import *\n",
    "start_inference(gpu=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29f43e9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T02:29:36.492640Z",
     "iopub.status.busy": "2025-07-07T02:29:36.492233Z",
     "iopub.status.idle": "2025-07-07T02:29:36.501179Z",
     "shell.execute_reply": "2025-07-07T02:29:36.499455Z"
    },
    "papermill": {
     "duration": 0.030607,
     "end_time": "2025-07-07T02:29:36.505654",
     "exception": false,
     "start_time": "2025-07-07T02:29:36.475047",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python --bg --proc infer_proc1\n",
    "from common_stuff import *\n",
    "start_inference(gpu=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3dd88e29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T02:29:36.569167Z",
     "iopub.status.busy": "2025-07-07T02:29:36.568615Z",
     "iopub.status.idle": "2025-07-07T02:29:36.585794Z",
     "shell.execute_reply": "2025-07-07T02:29:36.581898Z"
    },
    "papermill": {
     "duration": 0.042288,
     "end_time": "2025-07-07T02:29:36.588592",
     "exception": false,
     "start_time": "2025-07-07T02:29:36.546304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python --bg --proc infer_proc2\n",
    "from common_stuff import *\n",
    "start_inference(gpu=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f7a0f9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T02:29:36.667563Z",
     "iopub.status.busy": "2025-07-07T02:29:36.664158Z",
     "iopub.status.idle": "2025-07-07T02:29:36.693135Z",
     "shell.execute_reply": "2025-07-07T02:29:36.690409Z"
    },
    "papermill": {
     "duration": 0.072506,
     "end_time": "2025-07-07T02:29:36.697638",
     "exception": false,
     "start_time": "2025-07-07T02:29:36.625132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%python --bg --proc infer_proc3\n",
    "from common_stuff import *\n",
    "start_inference(gpu=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f9c7ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T02:29:36.777387Z",
     "iopub.status.busy": "2025-07-07T02:29:36.777024Z",
     "iopub.status.idle": "2025-07-07T08:11:12.677144Z",
     "shell.execute_reply": "2025-07-07T08:11:12.676500Z"
    },
    "papermill": {
     "duration": 20495.938614,
     "end_time": "2025-07-07T08:11:12.678672",
     "exception": false,
     "start_time": "2025-07-07T02:29:36.740058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "proc_exit_codes = await wait_for_subprocesses(\n",
    "    train_proc0, train_proc1, train_proc2, train_proc3,\n",
    "    infer_proc0, infer_proc1, infer_proc2, infer_proc3, \n",
    "    print_output=True or arc_test_set.is_fake)\n",
    "print(f'*** Subprocesses exit codes: {proc_exit_codes}')\n",
    "assert all(x==0 for x in proc_exit_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358b9d7f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-07T08:11:12.974416Z",
     "iopub.status.busy": "2025-07-07T08:11:12.973666Z",
     "iopub.status.idle": "2025-07-07T08:11:13.124832Z",
     "shell.execute_reply": "2025-07-07T08:11:13.124227Z"
    },
    "papermill": {
     "duration": 0.300607,
     "end_time": "2025-07-07T08:11:13.126227",
     "exception": false,
     "start_time": "2025-07-07T08:11:12.825620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# write submission 부분 수정\n",
    "from common_stuff import *\n",
    "with RemapCudaOOM():\n",
    "    model, formatter, dataset = None, MyFormatter(), None\n",
    "    decoder = Decoder(formatter, arc_test_set.split_multi_replies(), n_guesses=2, frac_score=True)\n",
    "    \n",
    "    # 모든 GPU의 결과를 병합\n",
    "    for gpu in range(4):\n",
    "        gpu_store = f\"{infer_temp_storage}_gpu{gpu}\" if multi_gpu_train else infer_temp_storage\n",
    "        if os.path.exists(gpu_store):\n",
    "            decoder.from_store(gpu_store)\n",
    "    \n",
    "    if use_aug_score or arc_test_set.is_fake: \n",
    "        decoder.calc_augmented_scores(model=model, store=score_temp_storage, **aug_score_params)\n",
    "    \n",
    "    submission = arc_test_set.get_submission(decoder.run_selection_algo(submission_select_algo))\n",
    "    with open('submission.json', 'w') as f: json.dump(submission, f)\n",
    "    if arc_test_set.is_fake:\n",
    "        decoder.benchmark_selection_algos(selection_algorithms)\n",
    "        with open('submission.json') as f: reload_submission = json.load(f)\n",
    "        print('*** Reload score:', arc_test_set.validate_submission(reload_submission))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaL4",
   "dataSources": [
    {
     "databundleVersionId": 11802066,
     "isSourceIdPinned": false,
     "sourceId": 91496,
     "sourceType": "competition"
    },
    {
     "datasetId": 5793177,
     "sourceId": 9515958,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 157175,
     "modelInstanceId": 134422,
     "sourceId": 158171,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30762,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 20598.572993,
   "end_time": "2025-07-07T08:11:13.689900",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-07T02:27:55.116907",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
